[
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "PretrainedConfig",
        "importPath": "transformers.configuration_utils",
        "description": "transformers.configuration_utils",
        "isExtraImport": true,
        "detail": "transformers.configuration_utils",
        "documentation": {}
    },
    {
        "label": "PretrainedConfig",
        "importPath": "transformers.configuration_utils",
        "description": "transformers.configuration_utils",
        "isExtraImport": true,
        "detail": "transformers.configuration_utils",
        "documentation": {}
    },
    {
        "label": "PretrainedConfig",
        "importPath": "transformers.configuration_utils",
        "description": "transformers.configuration_utils",
        "isExtraImport": true,
        "detail": "transformers.configuration_utils",
        "documentation": {}
    },
    {
        "label": "PretrainedConfig",
        "importPath": "transformers.configuration_utils",
        "description": "transformers.configuration_utils",
        "isExtraImport": true,
        "detail": "transformers.configuration_utils",
        "documentation": {}
    },
    {
        "label": "OnnxSeq2SeqConfigWithPast",
        "importPath": "transformers.onnx",
        "description": "transformers.onnx",
        "isExtraImport": true,
        "detail": "transformers.onnx",
        "documentation": {}
    },
    {
        "label": "OnnxSeq2SeqConfigWithPast",
        "importPath": "transformers.onnx",
        "description": "transformers.onnx",
        "isExtraImport": true,
        "detail": "transformers.onnx",
        "documentation": {}
    },
    {
        "label": "OnnxSeq2SeqConfigWithPast",
        "importPath": "transformers.onnx",
        "description": "transformers.onnx",
        "isExtraImport": true,
        "detail": "transformers.onnx",
        "documentation": {}
    },
    {
        "label": "OnnxSeq2SeqConfigWithPast",
        "importPath": "transformers.onnx",
        "description": "transformers.onnx",
        "isExtraImport": true,
        "detail": "transformers.onnx",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "DUMMY_INPUTS",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "DUMMY_MASK",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings_to_model_forward",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_torch_fx_proxy",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "replace_return_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "DUMMY_INPUTS",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "DUMMY_MASK",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings_to_model_forward",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_torch_fx_proxy",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "replace_return_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "DUMMY_INPUTS",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "DUMMY_MASK",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings_to_model_forward",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_torch_fx_proxy",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "replace_return_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "DUMMY_INPUTS",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "DUMMY_MASK",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "add_start_docstrings_to_model_forward",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_torch_fx_proxy",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "replace_return_docstrings",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "is_sentencepiece_available",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "transformers.utils",
        "description": "transformers.utils",
        "isExtraImport": true,
        "detail": "transformers.utils",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "ACT2FN",
        "importPath": "transformers.activations",
        "description": "transformers.activations",
        "isExtraImport": true,
        "detail": "transformers.activations",
        "documentation": {}
    },
    {
        "label": "ACT2FN",
        "importPath": "transformers.activations",
        "description": "transformers.activations",
        "isExtraImport": true,
        "detail": "transformers.activations",
        "documentation": {}
    },
    {
        "label": "ACT2FN",
        "importPath": "transformers.activations",
        "description": "transformers.activations",
        "isExtraImport": true,
        "detail": "transformers.activations",
        "documentation": {}
    },
    {
        "label": "ACT2FN",
        "importPath": "transformers.activations",
        "description": "transformers.activations",
        "isExtraImport": true,
        "detail": "transformers.activations",
        "documentation": {}
    },
    {
        "label": "BaseModelOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "BaseModelOutputWithPastAndCrossAttentions",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "Seq2SeqLMOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "Seq2SeqModelOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "BaseModelOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "BaseModelOutputWithPastAndCrossAttentions",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "Seq2SeqLMOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "Seq2SeqModelOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "BaseModelOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "BaseModelOutputWithPastAndCrossAttentions",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "Seq2SeqLMOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "Seq2SeqModelOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "BaseModelOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "BaseModelOutputWithPastAndCrossAttentions",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "Seq2SeqLMOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "Seq2SeqModelOutput",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "PreTrainedModel",
        "importPath": "transformers.modeling_utils",
        "description": "transformers.modeling_utils",
        "isExtraImport": true,
        "detail": "transformers.modeling_utils",
        "documentation": {}
    },
    {
        "label": "ALL_LAYERNORM_LAYERS",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "find_pruneable_heads_and_indices",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "prune_linear_layer",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "ALL_LAYERNORM_LAYERS",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "find_pruneable_heads_and_indices",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "prune_linear_layer",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "ALL_LAYERNORM_LAYERS",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "find_pruneable_heads_and_indices",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "prune_linear_layer",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "ALL_LAYERNORM_LAYERS",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "find_pruneable_heads_and_indices",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "prune_linear_layer",
        "importPath": "transformers.pytorch_utils",
        "description": "transformers.pytorch_utils",
        "isExtraImport": true,
        "detail": "transformers.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "assert_device_map",
        "importPath": "transformers.utils.model_parallel_utils",
        "description": "transformers.utils.model_parallel_utils",
        "isExtraImport": true,
        "detail": "transformers.utils.model_parallel_utils",
        "documentation": {}
    },
    {
        "label": "get_device_map",
        "importPath": "transformers.utils.model_parallel_utils",
        "description": "transformers.utils.model_parallel_utils",
        "isExtraImport": true,
        "detail": "transformers.utils.model_parallel_utils",
        "documentation": {}
    },
    {
        "label": "assert_device_map",
        "importPath": "transformers.utils.model_parallel_utils",
        "description": "transformers.utils.model_parallel_utils",
        "isExtraImport": true,
        "detail": "transformers.utils.model_parallel_utils",
        "documentation": {}
    },
    {
        "label": "get_device_map",
        "importPath": "transformers.utils.model_parallel_utils",
        "description": "transformers.utils.model_parallel_utils",
        "isExtraImport": true,
        "detail": "transformers.utils.model_parallel_utils",
        "documentation": {}
    },
    {
        "label": "assert_device_map",
        "importPath": "transformers.utils.model_parallel_utils",
        "description": "transformers.utils.model_parallel_utils",
        "isExtraImport": true,
        "detail": "transformers.utils.model_parallel_utils",
        "documentation": {}
    },
    {
        "label": "get_device_map",
        "importPath": "transformers.utils.model_parallel_utils",
        "description": "transformers.utils.model_parallel_utils",
        "isExtraImport": true,
        "detail": "transformers.utils.model_parallel_utils",
        "documentation": {}
    },
    {
        "label": "assert_device_map",
        "importPath": "transformers.utils.model_parallel_utils",
        "description": "transformers.utils.model_parallel_utils",
        "isExtraImport": true,
        "detail": "transformers.utils.model_parallel_utils",
        "documentation": {}
    },
    {
        "label": "get_device_map",
        "importPath": "transformers.utils.model_parallel_utils",
        "description": "transformers.utils.model_parallel_utils",
        "isExtraImport": true,
        "detail": "transformers.utils.model_parallel_utils",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "wraps",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentDefaultsHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentDefaultsHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentDefaultsHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentDefaultsHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Namespace",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "transformers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "transformers",
        "description": "transformers",
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5TokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PreTrainedTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForSeq2SeqLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "wandb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wandb",
        "description": "wandb",
        "detail": "wandb",
        "documentation": {}
    },
    {
        "label": "torch_geometric",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch_geometric",
        "description": "torch_geometric",
        "detail": "torch_geometric",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "torch_geometric.data",
        "description": "torch_geometric.data",
        "isExtraImport": true,
        "detail": "torch_geometric.data",
        "documentation": {}
    },
    {
        "label": "torch_geometric.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch_geometric.nn",
        "description": "torch_geometric.nn",
        "detail": "torch_geometric.nn",
        "documentation": {}
    },
    {
        "label": "add_args_shared",
        "importPath": "experiments.encoder.relation_prediction.train_LM",
        "description": "experiments.encoder.relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "chunker",
        "importPath": "experiments.encoder.relation_prediction.train_LM",
        "description": "experiments.encoder.relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_accuracy",
        "importPath": "experiments.encoder.relation_prediction.train_LM",
        "description": "experiments.encoder.relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "experiments.encoder.relation_prediction.train_LM",
        "description": "experiments.encoder.relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "GraphT5Classifier",
        "importPath": "models.graph_T5.classifier",
        "description": "models.graph_T5.classifier",
        "isExtraImport": true,
        "detail": "models.graph_T5.classifier",
        "documentation": {}
    },
    {
        "label": "GraphT5Classifier",
        "importPath": "models.graph_T5.classifier",
        "description": "models.graph_T5.classifier",
        "isExtraImport": true,
        "detail": "models.graph_T5.classifier",
        "documentation": {}
    },
    {
        "label": "DualGraphT5Classifier",
        "importPath": "models.graph_T5.classifier",
        "description": "models.graph_T5.classifier",
        "isExtraImport": true,
        "detail": "models.graph_T5.classifier",
        "documentation": {}
    },
    {
        "label": "GraphT5Classifier",
        "importPath": "models.graph_T5.classifier",
        "description": "models.graph_T5.classifier",
        "isExtraImport": true,
        "detail": "models.graph_T5.classifier",
        "documentation": {}
    },
    {
        "label": "DualGraphT5Classifier",
        "importPath": "models.graph_T5.classifier",
        "description": "models.graph_T5.classifier",
        "isExtraImport": true,
        "detail": "models.graph_T5.classifier",
        "documentation": {}
    },
    {
        "label": "GraphT5Classifier",
        "importPath": "models.graph_T5.classifier",
        "description": "models.graph_T5.classifier",
        "isExtraImport": true,
        "detail": "models.graph_T5.classifier",
        "documentation": {}
    },
    {
        "label": "T5TokenizerFast",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5TokenizerFast",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5TokenizerFast",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5TokenizerFast",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "T5TokenizerFast",
        "importPath": "models.graph_T5.graph_t5",
        "description": "models.graph_T5.graph_t5",
        "isExtraImport": true,
        "detail": "models.graph_T5.graph_t5",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_set_of_triplets",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_set_of_triplets",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "add_text_to_graph_data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_set_of_triplets",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "add_text_to_graph_data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_dummy_graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_dummy_graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "_get_str2tok",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "add_text_to_graph_data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "add_text_to_graph_data",
        "importPath": "models.graph_T5.wrapper_functions",
        "description": "models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "h5py",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "h5py",
        "description": "h5py",
        "detail": "h5py",
        "documentation": {}
    },
    {
        "label": "str2int",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2bool",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2path",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2criterion",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2logging_level",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "OpenData",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "data_to_dataT5",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_data_instances",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_batch",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "chunker",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_accuracy",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "run_eval_epoch",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "reset_params",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_metrics",
        "importPath": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "isExtraImport": true,
        "detail": "experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "importPath": "transformers.models.t5.configuration_t5",
        "description": "transformers.models.t5.configuration_t5",
        "isExtraImport": true,
        "detail": "transformers.models.t5.configuration_t5",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "copyfile",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "copyfile",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "sentencepiece",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sentencepiece",
        "description": "sentencepiece",
        "detail": "sentencepiece",
        "documentation": {}
    },
    {
        "label": "PreTrainedTokenizer",
        "importPath": "transformers.tokenization_utils",
        "description": "transformers.tokenization_utils",
        "isExtraImport": true,
        "detail": "transformers.tokenization_utils",
        "documentation": {}
    },
    {
        "label": "PreTrainedTokenizerFast",
        "importPath": "transformers.tokenization_utils_fast",
        "description": "transformers.tokenization_utils_fast",
        "isExtraImport": true,
        "detail": "transformers.tokenization_utils_fast",
        "documentation": {}
    },
    {
        "label": "igraph",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "igraph",
        "description": "igraph",
        "detail": "igraph",
        "documentation": {}
    },
    {
        "label": "SentenceTransformer",
        "importPath": "sentence_transformers",
        "description": "sentence_transformers",
        "isExtraImport": true,
        "detail": "sentence_transformers",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "SPARQLWrapper",
        "importPath": "SPARQLWrapper",
        "description": "SPARQLWrapper",
        "isExtraImport": true,
        "detail": "SPARQLWrapper",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "GraphT5ForConditionalGeneration",
        "importPath": "models.graph_T5.autoregressive_GLM",
        "description": "models.graph_T5.autoregressive_GLM",
        "isExtraImport": true,
        "detail": "models.graph_T5.autoregressive_GLM",
        "documentation": {}
    },
    {
        "label": "T5Tokenizer",
        "importPath": "seminars.kolber.GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "description": "seminars.kolber.GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "isExtraImport": true,
        "detail": "seminars.kolber.GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "documentation": {}
    },
    {
        "label": "r2nl",
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "isExtraImport": true,
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "ModelAndTokenizer",
        "importPath": "general_utils",
        "description": "general_utils",
        "isExtraImport": true,
        "detail": "general_utils",
        "documentation": {}
    },
    {
        "label": "ModelAndTokenizer",
        "importPath": "general_utils",
        "description": "general_utils",
        "isExtraImport": true,
        "detail": "general_utils",
        "documentation": {}
    },
    {
        "label": "decode_tokens",
        "importPath": "general_utils",
        "description": "general_utils",
        "isExtraImport": true,
        "detail": "general_utils",
        "documentation": {}
    },
    {
        "label": "make_inputs",
        "importPath": "general_utils",
        "description": "general_utils",
        "isExtraImport": true,
        "detail": "general_utils",
        "documentation": {}
    },
    {
        "label": "set_hs_patch_hooks_glm",
        "importPath": "patchscopes_utils",
        "description": "patchscopes_utils",
        "isExtraImport": true,
        "detail": "patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "data.load_data",
        "description": "data.load_data",
        "isExtraImport": true,
        "detail": "data.load_data",
        "documentation": {}
    },
    {
        "label": "evaluate_patch_glm_accuracy",
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "isExtraImport": true,
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.configuration_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.configuration_t5",
        "peekOfCode": "class T5Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`T5Model`] or a [`TFT5Model`]. It is used to\n    instantiate a T5 model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the T5\n    [t5-small](https://huggingface.co/t5-small) architecture.\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Arguments:\n        vocab_size (`int`, *optional*, defaults to 32128):",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.configuration_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.configuration_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.configuration_t5",
        "peekOfCode": "logger = logging.get_logger(__name__)\n## FROM-T5\n# T5_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n#     \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/config.json\",\n#     \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/config.json\",\n#     \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/config.json\",\n#     \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/config.json\",\n#     \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/config.json\",\n# }\nclass T5Config(PretrainedConfig):",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.configuration_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerNorm",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5DenseActDense",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5DenseActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n    def forward(self, hidden_states):\n        hidden_states = self.wi(hidden_states)\n        hidden_states = self.act(hidden_states)",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5DenseGatedActDense",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5DenseGatedActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n    def forward(self, hidden_states):\n        hidden_gelu = self.act(self.wi_0(hidden_states))",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerFF",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5LayerFF(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        if config.is_gated_act:\n            self.DenseReluDense = T5DenseGatedActDense(config)\n        else:\n            self.DenseReluDense = T5DenseActDense(config)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(self, hidden_states):",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Attention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5Attention(nn.Module):\n    def __init__(self, config: T5Config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.relative_attention_num_additional_buckets = config.relative_attention_num_additional_buckets if 'relative_attention_num_additional_buckets' in config.__dict__.keys() else 0\n        self.relative_attention_max_distance = config.relative_attention_max_distance\n        self.d_model = config.d_model\n        self.key_value_proj_dim = config.d_kv",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerSelfAttention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5LayerSelfAttention(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerCrossAttention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5LayerCrossAttention(nn.Module):\n    def __init__(self, config):\n        raise NotImplementedError(\"might need adjustments for GLM\")\n        super().__init__()\n        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(\n        self,\n        hidden_states,",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Block",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5Block(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.layer = nn.ModuleList()\n        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n        if self.is_decoder:\n            self.layer.append(T5LayerCrossAttention(config))\n        self.layer.append(T5LayerFF(config))\n    def forward(",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5PreTrainedModel",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5PreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n    config_class = T5Config\n    load_tf_weights = load_tf_weights_in_t5\n    base_model_prefix = \"transformer\"\n    is_parallelizable = True\n    supports_gradient_checkpointing = True",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Stack",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5Stack(T5PreTrainedModel):\n    def __init__(self, config, embed_tokens=None):\n        super().__init__(config)\n        self.embed_tokens = embed_tokens\n        self.is_decoder = config.is_decoder\n        self.block = nn.ModuleList(\n            [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n        )\n        self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Model",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5Model(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n    def __init__(self, config: T5Config):\n        raise NotImplementedError(\"might need adjustments for GLM\")",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5ForConditionalGeneration(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n        r\"lm_head.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n    def __init__(self, config: T5Config):",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class T5EncoderModel(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"encoder.embed_tokens.weight\"]\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n        encoder_config = copy.deepcopy(config)\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n        # Initialize weights and apply final processing",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "GraphT5Classifier",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class GraphT5Classifier(PreTrainedModel):\n    config_class = T5Config\n    def __init__(\n        self,\n        config: T5Config,\n    ):\n        super().__init__(config=config)\n        self.config = config\n        self.tokenizer = T5Tokenizer.from_pretrained(self.config.modelsize, model_max_length=self.config.model_max_length)\n        self.t5model = T5EncoderModel.from_pretrained(self.config.modelsize, config=config, ignore_mismatched_sizes=True)  # when intialiting the model with .from_pretrained, the weights are loaded from the pretrained model, so the t5 parameters are not actually used in that case. Loading them here is unnecessary overhead. ",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "DualGraphT5Classifier",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "class DualGraphT5Classifier(PreTrainedModel):\n    \"\"\"\n    Same as GraphT5Classifier, but with two classification heads\n    \"\"\"\n    config_class = T5Config\n    def __init__(\n        self,\n        config: T5Config,\n    ):\n        super().__init__(config=config)",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "load_tf_weights_in_t5",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "def load_tf_weights_in_t5(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    raise NotImplementedError(\"NOT TESTED; might need adjustments for GLM\")\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "logger = transformers_logging.get_logger(__name__)\n_CONFIG_FOR_DOC = \"T5Config\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "_CONFIG_FOR_DOC",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "_CONFIG_FOR_DOC = \"T5Config\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "_CHECKPOINT_FOR_DOC",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_PRETRAINED_MODEL_ARCHIVE_LIST",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "T5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",\n    \"t5-11b\",\n    # See all T5 models at https://huggingface.co/models?filter=t5\n]\n####################################################\n# This is a conversion method from TF 1.0 to PyTorch",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "PARALLELIZE_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "PARALLELIZE_DOCSTRING = r\"\"\"\n    This is an experimental feature and is a subject to change at a moment's notice.\n    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n    it will evenly distribute blocks across all devices.\n    Args:\n        device_map (`Dict[int, list]`, optional, defaults to None):\n            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n            have fewer attention modules mapped to it than other devices. For reference, the t5 models have the\n            following number of attention modules:",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "DEPARALLELIZE_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "DEPARALLELIZE_DOCSTRING = r\"\"\"\n    Moves the model to cpu from a model parallel state.\n    Example:\n    ```python\n    # On a 4 GPU machine with t5-3b:\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n    device_map = {\n        0: [0, 1, 2],\n        1: [3, 4, 5, 6, 7, 8, 9],\n        2: [10, 11, 12, 13, 14, 15, 16],",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_START_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "T5_START_DOCSTRING = r\"\"\"\n    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\n    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a\n    text-to-text denoising generative setting.\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "T5_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n            [What are input IDs?](../glossary#input-ids)\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_ENCODER_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "T5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "__HEAD_MASK_WARNING_MSG",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "peekOfCode": "__HEAD_MASK_WARNING_MSG = \"\"\"\nThe input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\nIf you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\nnum_heads)`.\n\"\"\"\n@add_start_docstrings(\n    \"The bare T5 Model transformer outputting raw hidden-states without any specific head on top.\",\n    T5_START_DOCSTRING,\n)",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.modeling_t5",
        "documentation": {}
    },
    {
        "label": "Graph",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "peekOfCode": "class Graph():\n    \"\"\"\n    A graph class.\n    :param g: A list of tuples, where each tuple is a triple (head, r, tail).\n    \"\"\"\n    def __init__(\n            self, \n            g: List[Tuple[str,str,str]] = []\n        ):\n        self.g = g",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "peekOfCode": "class Data(Namespace):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.__dict__.update(kwargs)\ndef get_dummy_graph(num_triplets:int=3) -> Graph:\n    g = [\n        (\"dog\", \"IsA\", \"animal\"),\n        (\"cat\", \"IsA\", \"animal\"),\n        (\"black poodle\", \"IsA\", \"dog\"),\n        (\"black cat\", \"IsA\", \"cat\"),",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "DataProcessor",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "peekOfCode": "class DataProcessor():\n    @staticmethod\n    def encode_graph(tokenizer, g:Union[Graph,list[tuple[str,str,str]]], text:Optional[str]=None, how:Literal['global', 'local']='global', eos:str=\"False\")->Data:\n        \"\"\"\n        convert graph to suitable input for the model. \n        :param tokenizer: tokenizer\n        :param g: graph\n        :param text: text to add to the graph. Can be None if no text should be added. \n        :param how: how to represent the graph. Can be 'local' or 'global' for lGLM and gGLM respectively.\n        :param eos: end-of-sequence token. Can be `False` for not using an eos token. This is the method used in the paper. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph. `unidirectional` means that the eos token is connected to every node in the graph (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which perceives locality when using the local GLM",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_dummy_graph",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "peekOfCode": "def get_dummy_graph(num_triplets:int=3) -> Graph:\n    g = [\n        (\"dog\", \"IsA\", \"animal\"),\n        (\"cat\", \"IsA\", \"animal\"),\n        (\"black poodle\", \"IsA\", \"dog\"),\n        (\"black cat\", \"IsA\", \"cat\"),\n    ]\n    assert num_triplets <=4, \"num_triplets must be <= 4\"\n    g = g[:num_triplets]\n    g = Graph(g)",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "r2nl",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "peekOfCode": "def r2nl(r: str) -> str:\n    \"\"\"\n    Convert a relation to a natural language string. Can be used to implement necessary changes in the data.\n    \"\"\"\n    return r\ndef _get_str2tok(g:Graph, tokenizer: T5Tokenizer) -> dict[str, list[int]]:\n    \"\"\"\n    Get a dictionary that maps strings to tokens.\n    \"\"\"\n    # tokenize concepts and relations",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "peekOfCode": "def graph_to_graphT5(g:Graph, tokenizer:T5Tokenizer, how:str, eos:str)->Data:\n    \"\"\"\n    Convert a graph to a graphT5 input.\n    :param g: graph\n    :param tokenizer: tokenizer\n    :param how: how to represent the graph. Can be 'local' or 'global' for lGLM and gGLM respectively.\n    :param eos: end-of-sequence token. Can be `False` for not using an eos token. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph, with a relative position of positive infinity (from node to eos) or negative infinity (from eos to node). `unidirectional` means that the eos token is connected to every node in the graph with a relative position of positive infinity (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which perceives locality when using the local GLM\n    \"\"\"\n    if not isinstance(g, Graph):\n        g = Graph(g)",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "peekOfCode": "def get_embedding(\n        sequence_embedding: torch.Tensor,\n        indices: Dict[str, List[Tuple[int, int]]],\n        concept: str,\n        embedding_aggregation: str = \"mean\",\n    ):\n    \"\"\"\n    Returns the embedding of a concept.\n    :param sequence_embedding: the embedding of the whole sequence. shape: (sequence_length, embedding_size)\n    :param indices: dictionary mapping each node to its start-index and end- in the sequence. Keys are nodes, values are lists of tuples (start_index, end_index). The lists have a length of 1 for concepts.",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "add_text_to_graph_data",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "description": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "peekOfCode": "def add_text_to_graph_data(data, text, tokenizer, use_text):\n    if use_text in {'False', '', False, None}:\n        return None\n    text_seq = torch.tensor(tokenizer(text, padding=False)['input_ids']).unsqueeze(0)\n    new_input_ids = torch.cat([data.input_ids, text_seq], dim=1)\n    old_seq_len = data.input_ids.shape[1]\n    text_seq_len = text_seq.shape[1]\n    new_seq_len = new_input_ids.shape[1]\n    new_is_graph = torch.zeros(size=(1, new_seq_len), dtype=torch.bool)\n    new_is_graph[:, :old_seq_len] = True",
        "detail": ".cache.models--plenz--GLM-flan-t5-base.snapshots.f9e096eef18d7a2ac1996b69015a06ebb0b1b177.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.configuration_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.configuration_t5",
        "peekOfCode": "class T5Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`T5Model`] or a [`TFT5Model`]. It is used to\n    instantiate a T5 model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the T5\n    [t5-small](https://huggingface.co/t5-small) architecture.\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Arguments:\n        vocab_size (`int`, *optional*, defaults to 32128):",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.configuration_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.configuration_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.configuration_t5",
        "peekOfCode": "logger = logging.get_logger(__name__)\n## FROM-T5\n# T5_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n#     \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/config.json\",\n#     \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/config.json\",\n#     \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/config.json\",\n#     \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/config.json\",\n#     \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/config.json\",\n# }\nclass T5Config(PretrainedConfig):",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.configuration_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerNorm",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5DenseActDense",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5DenseActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n    def forward(self, hidden_states):\n        hidden_states = self.wi(hidden_states)\n        hidden_states = self.act(hidden_states)",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5DenseGatedActDense",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5DenseGatedActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n    def forward(self, hidden_states):\n        hidden_gelu = self.act(self.wi_0(hidden_states))",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerFF",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5LayerFF(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        if config.is_gated_act:\n            self.DenseReluDense = T5DenseGatedActDense(config)\n        else:\n            self.DenseReluDense = T5DenseActDense(config)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(self, hidden_states):",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Attention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5Attention(nn.Module):\n    def __init__(self, config: T5Config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.relative_attention_num_additional_buckets = config.relative_attention_num_additional_buckets if 'relative_attention_num_additional_buckets' in config.__dict__.keys() else 0\n        self.relative_attention_max_distance = config.relative_attention_max_distance\n        self.d_model = config.d_model\n        self.key_value_proj_dim = config.d_kv",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerSelfAttention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5LayerSelfAttention(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerCrossAttention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5LayerCrossAttention(nn.Module):\n    def __init__(self, config):\n        raise NotImplementedError(\"might need adjustments for GLM\")\n        super().__init__()\n        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(\n        self,\n        hidden_states,",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Block",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5Block(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.layer = nn.ModuleList()\n        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n        if self.is_decoder:\n            self.layer.append(T5LayerCrossAttention(config))\n        self.layer.append(T5LayerFF(config))\n    def forward(",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5PreTrainedModel",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5PreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n    config_class = T5Config\n    load_tf_weights = load_tf_weights_in_t5\n    base_model_prefix = \"transformer\"\n    is_parallelizable = True\n    supports_gradient_checkpointing = True",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Stack",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5Stack(T5PreTrainedModel):\n    def __init__(self, config, embed_tokens=None):\n        super().__init__(config)\n        self.embed_tokens = embed_tokens\n        self.is_decoder = config.is_decoder\n        self.block = nn.ModuleList(\n            [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n        )\n        self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Model",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5Model(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n    def __init__(self, config: T5Config):\n        raise NotImplementedError(\"might need adjustments for GLM\")",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5ForConditionalGeneration(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n        r\"lm_head.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n    def __init__(self, config: T5Config):",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class T5EncoderModel(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"encoder.embed_tokens.weight\"]\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n        encoder_config = copy.deepcopy(config)\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n        # Initialize weights and apply final processing",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "GraphT5Classifier",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class GraphT5Classifier(PreTrainedModel):\n    config_class = T5Config\n    def __init__(\n        self,\n        config: T5Config,\n    ):\n        super().__init__(config=config)\n        self.config = config\n        self.tokenizer = T5Tokenizer.from_pretrained(self.config.modelsize, model_max_length=self.config.model_max_length)\n        self.t5model = T5EncoderModel.from_pretrained(self.config.modelsize, config=config, ignore_mismatched_sizes=True)  # when intialiting the model with .from_pretrained, the weights are loaded from the pretrained model, so the t5 parameters are not actually used in that case. Loading them here is unnecessary overhead. ",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "DualGraphT5Classifier",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "class DualGraphT5Classifier(PreTrainedModel):\n    \"\"\"\n    Same as GraphT5Classifier, but with two classification heads\n    \"\"\"\n    config_class = T5Config\n    def __init__(\n        self,\n        config: T5Config,\n    ):\n        super().__init__(config=config)",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "load_tf_weights_in_t5",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "def load_tf_weights_in_t5(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    raise NotImplementedError(\"NOT TESTED; might need adjustments for GLM\")\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "logger = transformers_logging.get_logger(__name__)\n_CONFIG_FOR_DOC = \"T5Config\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "_CONFIG_FOR_DOC",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "_CONFIG_FOR_DOC = \"T5Config\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "_CHECKPOINT_FOR_DOC",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_PRETRAINED_MODEL_ARCHIVE_LIST",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "T5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",\n    \"t5-11b\",\n    # See all T5 models at https://huggingface.co/models?filter=t5\n]\n####################################################\n# This is a conversion method from TF 1.0 to PyTorch",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "PARALLELIZE_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "PARALLELIZE_DOCSTRING = r\"\"\"\n    This is an experimental feature and is a subject to change at a moment's notice.\n    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n    it will evenly distribute blocks across all devices.\n    Args:\n        device_map (`Dict[int, list]`, optional, defaults to None):\n            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n            have fewer attention modules mapped to it than other devices. For reference, the t5 models have the\n            following number of attention modules:",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "DEPARALLELIZE_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "DEPARALLELIZE_DOCSTRING = r\"\"\"\n    Moves the model to cpu from a model parallel state.\n    Example:\n    ```python\n    # On a 4 GPU machine with t5-3b:\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n    device_map = {\n        0: [0, 1, 2],\n        1: [3, 4, 5, 6, 7, 8, 9],\n        2: [10, 11, 12, 13, 14, 15, 16],",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_START_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "T5_START_DOCSTRING = r\"\"\"\n    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\n    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a\n    text-to-text denoising generative setting.\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "T5_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n            [What are input IDs?](../glossary#input-ids)\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_ENCODER_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "T5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "__HEAD_MASK_WARNING_MSG",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "peekOfCode": "__HEAD_MASK_WARNING_MSG = \"\"\"\nThe input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\nIf you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\nnum_heads)`.\n\"\"\"\n@add_start_docstrings(\n    \"The bare T5 Model transformer outputting raw hidden-states without any specific head on top.\",\n    T5_START_DOCSTRING,\n)",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.modeling_t5",
        "documentation": {}
    },
    {
        "label": "Graph",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "peekOfCode": "class Graph():\n    \"\"\"\n    A graph class.\n    :param g: A list of tuples, where each tuple is a triple (head, r, tail).\n    \"\"\"\n    def __init__(\n            self, \n            g: List[Tuple[str,str,str]] = []\n        ):\n        self.g = g",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "peekOfCode": "class Data(Namespace):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.__dict__.update(kwargs)\ndef get_dummy_graph(num_triplets:int=3) -> Graph:\n    g = [\n        (\"dog\", \"IsA\", \"animal\"),\n        (\"cat\", \"IsA\", \"animal\"),\n        (\"black poodle\", \"IsA\", \"dog\"),\n        (\"black cat\", \"IsA\", \"cat\"),",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "DataProcessor",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "peekOfCode": "class DataProcessor():\n    @staticmethod\n    def encode_graph(tokenizer, g:Union[Graph,list[tuple[str,str,str]]], text:Optional[str]=None, how:Literal['global', 'local']='global', eos:str=\"False\")->Data:\n        \"\"\"\n        convert graph to suitable input for the model. \n        :param tokenizer: tokenizer\n        :param g: graph\n        :param text: text to add to the graph. Can be None if no text should be added. \n        :param how: how to represent the graph. Can be 'local' or 'global' for lGLM and gGLM respectively.\n        :param eos: end-of-sequence token. Can be `False` for not using an eos token. This is the method used in the paper. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph. `unidirectional` means that the eos token is connected to every node in the graph (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which perceives locality when using the local GLM",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_dummy_graph",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "peekOfCode": "def get_dummy_graph(num_triplets:int=3) -> Graph:\n    g = [\n        (\"dog\", \"IsA\", \"animal\"),\n        (\"cat\", \"IsA\", \"animal\"),\n        (\"black poodle\", \"IsA\", \"dog\"),\n        (\"black cat\", \"IsA\", \"cat\"),\n    ]\n    assert num_triplets <=4, \"num_triplets must be <= 4\"\n    g = g[:num_triplets]\n    g = Graph(g)",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "r2nl",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "peekOfCode": "def r2nl(r: str) -> str:\n    \"\"\"\n    Convert a relation to a natural language string. Can be used to implement necessary changes in the data.\n    \"\"\"\n    return r\ndef _get_str2tok(g:Graph, tokenizer: T5Tokenizer) -> dict[str, list[int]]:\n    \"\"\"\n    Get a dictionary that maps strings to tokens.\n    \"\"\"\n    # tokenize concepts and relations",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "peekOfCode": "def graph_to_graphT5(g:Graph, tokenizer:T5Tokenizer, how:str, eos:str)->Data:\n    \"\"\"\n    Convert a graph to a graphT5 input.\n    :param g: graph\n    :param tokenizer: tokenizer\n    :param how: how to represent the graph. Can be 'local' or 'global' for lGLM and gGLM respectively.\n    :param eos: end-of-sequence token. Can be `False` for not using an eos token. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph, with a relative position of positive infinity (from node to eos) or negative infinity (from eos to node). `unidirectional` means that the eos token is connected to every node in the graph with a relative position of positive infinity (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which perceives locality when using the local GLM\n    \"\"\"\n    if not isinstance(g, Graph):\n        g = Graph(g)",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "peekOfCode": "def get_embedding(\n        sequence_embedding: torch.Tensor,\n        indices: Dict[str, List[Tuple[int, int]]],\n        concept: str,\n        embedding_aggregation: str = \"mean\",\n    ):\n    \"\"\"\n    Returns the embedding of a concept.\n    :param sequence_embedding: the embedding of the whole sequence. shape: (sequence_length, embedding_size)\n    :param indices: dictionary mapping each node to its start-index and end- in the sequence. Keys are nodes, values are lists of tuples (start_index, end_index). The lists have a length of 1 for concepts.",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "add_text_to_graph_data",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "peekOfCode": "def add_text_to_graph_data(data, text, tokenizer, use_text):\n    if use_text in {'False', '', False, None}:\n        return None\n    text_seq = torch.tensor(tokenizer(text, padding=False)['input_ids']).unsqueeze(0)\n    new_input_ids = torch.cat([data.input_ids, text_seq], dim=1)\n    old_seq_len = data.input_ids.shape[1]\n    text_seq_len = text_seq.shape[1]\n    new_seq_len = new_input_ids.shape[1]\n    new_is_graph = torch.zeros(size=(1, new_seq_len), dtype=torch.bool)\n    new_is_graph[:, :old_seq_len] = True",
        "detail": ".cache.models--plenz--GLM-t5-3b.snapshots.956c48e45bdf367cee263bf32494b372aae7d167.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.configuration_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.configuration_t5",
        "peekOfCode": "class T5Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`T5Model`] or a [`TFT5Model`]. It is used to\n    instantiate a T5 model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the T5\n    [t5-small](https://huggingface.co/t5-small) architecture.\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Arguments:\n        vocab_size (`int`, *optional*, defaults to 32128):",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.configuration_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.configuration_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.configuration_t5",
        "peekOfCode": "logger = logging.get_logger(__name__)\n## FROM-T5\n# T5_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n#     \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/config.json\",\n#     \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/config.json\",\n#     \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/config.json\",\n#     \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/config.json\",\n#     \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/config.json\",\n# }\nclass T5Config(PretrainedConfig):",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.configuration_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerNorm",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5DenseActDense",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5DenseActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n    def forward(self, hidden_states):\n        hidden_states = self.wi(hidden_states)\n        hidden_states = self.act(hidden_states)",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5DenseGatedActDense",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5DenseGatedActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n    def forward(self, hidden_states):\n        hidden_gelu = self.act(self.wi_0(hidden_states))",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerFF",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5LayerFF(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        if config.is_gated_act:\n            self.DenseReluDense = T5DenseGatedActDense(config)\n        else:\n            self.DenseReluDense = T5DenseActDense(config)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(self, hidden_states):",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Attention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5Attention(nn.Module):\n    def __init__(self, config: T5Config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.relative_attention_num_additional_buckets = config.relative_attention_num_additional_buckets if 'relative_attention_num_additional_buckets' in config.__dict__.keys() else 0\n        self.relative_attention_max_distance = config.relative_attention_max_distance\n        self.d_model = config.d_model\n        self.key_value_proj_dim = config.d_kv",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerSelfAttention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5LayerSelfAttention(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerCrossAttention",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5LayerCrossAttention(nn.Module):\n    def __init__(self, config):\n        raise NotImplementedError(\"might need adjustments for GLM\")\n        super().__init__()\n        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(\n        self,\n        hidden_states,",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Block",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5Block(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.layer = nn.ModuleList()\n        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n        if self.is_decoder:\n            self.layer.append(T5LayerCrossAttention(config))\n        self.layer.append(T5LayerFF(config))\n    def forward(",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5PreTrainedModel",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5PreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n    config_class = T5Config\n    load_tf_weights = load_tf_weights_in_t5\n    base_model_prefix = \"transformer\"\n    is_parallelizable = True\n    supports_gradient_checkpointing = True",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Stack",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5Stack(T5PreTrainedModel):\n    def __init__(self, config, embed_tokens=None):\n        super().__init__(config)\n        self.embed_tokens = embed_tokens\n        self.is_decoder = config.is_decoder\n        self.block = nn.ModuleList(\n            [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n        )\n        self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Model",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5Model(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n    def __init__(self, config: T5Config):\n        raise NotImplementedError(\"might need adjustments for GLM\")",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5ForConditionalGeneration(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n        r\"lm_head.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n    def __init__(self, config: T5Config):",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class T5EncoderModel(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"encoder.embed_tokens.weight\"]\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n        encoder_config = copy.deepcopy(config)\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n        # Initialize weights and apply final processing",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "GraphT5Classifier",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class GraphT5Classifier(PreTrainedModel):\n    config_class = T5Config\n    def __init__(\n        self,\n        config: T5Config,\n    ):\n        super().__init__(config=config)\n        self.config = config\n        self.tokenizer = T5Tokenizer.from_pretrained(self.config.modelsize, model_max_length=self.config.model_max_length)\n        self.t5model = T5EncoderModel.from_pretrained(self.config.modelsize, config=config, ignore_mismatched_sizes=True)  # when intialiting the model with .from_pretrained, the weights are loaded from the pretrained model, so the t5 parameters are not actually used in that case. Loading them here is unnecessary overhead. ",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "DualGraphT5Classifier",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "class DualGraphT5Classifier(PreTrainedModel):\n    \"\"\"\n    Same as GraphT5Classifier, but with two classification heads\n    \"\"\"\n    config_class = T5Config\n    def __init__(\n        self,\n        config: T5Config,\n    ):\n        super().__init__(config=config)",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "load_tf_weights_in_t5",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "def load_tf_weights_in_t5(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    raise NotImplementedError(\"NOT TESTED; might need adjustments for GLM\")\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "logger = transformers_logging.get_logger(__name__)\n_CONFIG_FOR_DOC = \"T5Config\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "_CONFIG_FOR_DOC",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "_CONFIG_FOR_DOC = \"T5Config\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "_CHECKPOINT_FOR_DOC",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_PRETRAINED_MODEL_ARCHIVE_LIST",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "T5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",\n    \"t5-11b\",\n    # See all T5 models at https://huggingface.co/models?filter=t5\n]\n####################################################\n# This is a conversion method from TF 1.0 to PyTorch",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "PARALLELIZE_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "PARALLELIZE_DOCSTRING = r\"\"\"\n    This is an experimental feature and is a subject to change at a moment's notice.\n    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n    it will evenly distribute blocks across all devices.\n    Args:\n        device_map (`Dict[int, list]`, optional, defaults to None):\n            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n            have fewer attention modules mapped to it than other devices. For reference, the t5 models have the\n            following number of attention modules:",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "DEPARALLELIZE_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "DEPARALLELIZE_DOCSTRING = r\"\"\"\n    Moves the model to cpu from a model parallel state.\n    Example:\n    ```python\n    # On a 4 GPU machine with t5-3b:\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n    device_map = {\n        0: [0, 1, 2],\n        1: [3, 4, 5, 6, 7, 8, 9],\n        2: [10, 11, 12, 13, 14, 15, 16],",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_START_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "T5_START_DOCSTRING = r\"\"\"\n    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\n    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a\n    text-to-text denoising generative setting.\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "T5_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n            [What are input IDs?](../glossary#input-ids)\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_ENCODER_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "T5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "__HEAD_MASK_WARNING_MSG",
        "kind": 5,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "peekOfCode": "__HEAD_MASK_WARNING_MSG = \"\"\"\nThe input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\nIf you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\nnum_heads)`.\n\"\"\"\n@add_start_docstrings(\n    \"The bare T5 Model transformer outputting raw hidden-states without any specific head on top.\",\n    T5_START_DOCSTRING,\n)",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.modeling_t5",
        "documentation": {}
    },
    {
        "label": "Graph",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "peekOfCode": "class Graph():\n    \"\"\"\n    A graph class.\n    :param g: A list of tuples, where each tuple is a triple (head, r, tail).\n    \"\"\"\n    def __init__(\n            self, \n            g: List[Tuple[str,str,str]] = []\n        ):\n        self.g = g",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "peekOfCode": "class Data(Namespace):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.__dict__.update(kwargs)\ndef get_dummy_graph(num_triplets:int=3) -> Graph:\n    g = [\n        (\"dog\", \"IsA\", \"animal\"),\n        (\"cat\", \"IsA\", \"animal\"),\n        (\"black poodle\", \"IsA\", \"dog\"),\n        (\"black cat\", \"IsA\", \"cat\"),",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "DataProcessor",
        "kind": 6,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "peekOfCode": "class DataProcessor():\n    @staticmethod\n    def encode_graph(tokenizer, g:Union[Graph,list[tuple[str,str,str]]], text:Optional[str]=None, how:Literal['global', 'local']='global', eos:str=\"False\")->Data:\n        \"\"\"\n        convert graph to suitable input for the model. \n        :param tokenizer: tokenizer\n        :param g: graph\n        :param text: text to add to the graph. Can be None if no text should be added. \n        :param how: how to represent the graph. Can be 'local' or 'global' for lGLM and gGLM respectively.\n        :param eos: end-of-sequence token. Can be `False` for not using an eos token. This is the method used in the paper. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph. `unidirectional` means that the eos token is connected to every node in the graph (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which perceives locality when using the local GLM",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_dummy_graph",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "peekOfCode": "def get_dummy_graph(num_triplets:int=3) -> Graph:\n    g = [\n        (\"dog\", \"IsA\", \"animal\"),\n        (\"cat\", \"IsA\", \"animal\"),\n        (\"black poodle\", \"IsA\", \"dog\"),\n        (\"black cat\", \"IsA\", \"cat\"),\n    ]\n    assert num_triplets <=4, \"num_triplets must be <= 4\"\n    g = g[:num_triplets]\n    g = Graph(g)",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "r2nl",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "peekOfCode": "def r2nl(r: str) -> str:\n    \"\"\"\n    Convert a relation to a natural language string. Can be used to implement necessary changes in the data.\n    \"\"\"\n    return r\ndef _get_str2tok(g:Graph, tokenizer: T5Tokenizer) -> dict[str, list[int]]:\n    \"\"\"\n    Get a dictionary that maps strings to tokens.\n    \"\"\"\n    # tokenize concepts and relations",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "peekOfCode": "def graph_to_graphT5(g:Graph, tokenizer:T5Tokenizer, how:str, eos:str)->Data:\n    \"\"\"\n    Convert a graph to a graphT5 input.\n    :param g: graph\n    :param tokenizer: tokenizer\n    :param how: how to represent the graph. Can be 'local' or 'global' for lGLM and gGLM respectively.\n    :param eos: end-of-sequence token. Can be `False` for not using an eos token. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph, with a relative position of positive infinity (from node to eos) or negative infinity (from eos to node). `unidirectional` means that the eos token is connected to every node in the graph with a relative position of positive infinity (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which perceives locality when using the local GLM\n    \"\"\"\n    if not isinstance(g, Graph):\n        g = Graph(g)",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "peekOfCode": "def get_embedding(\n        sequence_embedding: torch.Tensor,\n        indices: Dict[str, List[Tuple[int, int]]],\n        concept: str,\n        embedding_aggregation: str = \"mean\",\n    ):\n    \"\"\"\n    Returns the embedding of a concept.\n    :param sequence_embedding: the embedding of the whole sequence. shape: (sequence_length, embedding_size)\n    :param indices: dictionary mapping each node to its start-index and end- in the sequence. Keys are nodes, values are lists of tuples (start_index, end_index). The lists have a length of 1 for concepts.",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "add_text_to_graph_data",
        "kind": 2,
        "importPath": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "description": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "peekOfCode": "def add_text_to_graph_data(data, text, tokenizer, use_text):\n    if use_text in {'False', '', False, None}:\n        return None\n    text_seq = torch.tensor(tokenizer(text, padding=False)['input_ids']).unsqueeze(0)\n    new_input_ids = torch.cat([data.input_ids, text_seq], dim=1)\n    old_seq_len = data.input_ids.shape[1]\n    text_seq_len = text_seq.shape[1]\n    new_seq_len = new_input_ids.shape[1]\n    new_is_graph = torch.zeros(size=(1, new_seq_len), dtype=torch.bool)\n    new_is_graph[:, :old_seq_len] = True",
        "detail": ".cache.models--plenz--GLM-t5-large.snapshots.17fdd2e4e0d81cd9d547940a3c07042f5142159f.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "GNN",
        "kind": 6,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "class GNN(nn.Module):\n    def __init__(\n        self, \n        gnn_layer: str, \n        num_layers: int, \n        in_channels: int,\n        hidden_channels: int, \n        out_channels: int,\n        activation: str,\n        dropout: float,",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "add_args",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "def add_args(parser: ArgumentParser):\n    parser.add_argument(\n        \"--gnn_layer\", \n        type=str, \n        required=False, \n        default='GCNConv', \n        help=\"GNN layer to be used\"\n    )\n    parser.add_argument(\n        \"--num_layers\", ",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "get_load_edge_attribute",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "def get_load_edge_attribute(gnn_layer:str):\n    no_need_edge_attribute = ['GCNConv', 'GATConv', 'RGCNConv', 'RGATConv']\n    need_edge_attribute = []\n    if gnn_layer in no_need_edge_attribute:\n        return False\n    elif gnn_layer in need_edge_attribute:\n        return True\n    else:\n        raise ValueError(f\"Unknown gnn_layer {gnn_layer}\")\ndef parse_args(parser: ArgumentParser):",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "def parse_args(parser: ArgumentParser):\n    args = parser.parse_args()\n    args.load_edge_attribute = get_load_edge_attribute(args.gnn_layer)\n    args.fn_label_to_index = Path(f'data/knowledgegraph/conceptnet/subgraphs_{args.dataset_construction}/num_neighbors=[1,2,2,2,2]/label2index.json')\n    return args\ndef run_eval_epoch(model:gnn.MessagePassing, data:List[Data], criterion:nn.Module, batch_size:int, device:str):\n    with torch.no_grad():\n        losses = []\n        accuracies = []\n        weights = []",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "run_eval_epoch",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "def run_eval_epoch(model:gnn.MessagePassing, data:List[Data], criterion:nn.Module, batch_size:int, device:str):\n    with torch.no_grad():\n        losses = []\n        accuracies = []\n        weights = []\n        for data_instances in chunker(data, batch_size):\n            # create batch\n            batch = gtc.data.Batch.from_data_list(data_instances)\n            batch = batch.to(device)\n            logits = model.forward(",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "run_train_epoch",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "def run_train_epoch(model:gnn.MessagePassing, data:List[Data], criterion:nn.Module, optimizer:torch.optim.Optimizer, batch_size:int, device:str):\n    losses = []\n    accuracies = []\n    weights = []\n    random.shuffle(data)\n    for i, data_instances in tqdm(enumerate(chunker(data, batch_size)), total=len(data)//batch_size):\n        # logging.info(f\"batch {i}\")\n        optimizer.zero_grad()\n        # create batch\n        batch = gtc.data.Batch.from_data_list(data_instances)  ",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "def load_data(kg:str, dataset_construction:str, radius:str, num_masked:str, modelsize:str, load_edge_attribute:bool):\n    dir = Path(f\"data/knowledgegraph/{kg}/relation_subgraphs_{dataset_construction}/num_neighbors=[1,2,2,2,2]/num_masked={num_masked}/radius={radius}/torch_geometric/encoder={modelsize}\")\n    if load_edge_attribute:\n        fn = Path(dir, 'with_edge_attribute.pt')\n    else:\n        fn = Path(dir, 'no_edge_attribute.pt')\n    data = torch.load(fn)\n    # add self loops and turn to undirected graph\n    for data_split in data.values():\n        for d in data_split:",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "def main(args):\n    if not args.device.startswith('cuda'):\n        logging.warning(f'using CPU {args.device}, training might be slow.')\n    else:\n        logging.info(f'using GPU {args.device}')\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    if args.device.startswith('cuda'):\n        torch.cuda.manual_seed(args.seed)\n    else:",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "GNN_LAYERS",
        "kind": 5,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "GNN_LAYERS = {\n    'RGCNConv': gnn.FastRGCNConv, \n    'RGATConv': gnn.RGATConv,\n    'GCNConv': gnn.GCNConv,\n    'GATConv': gnn.GATConv,\n}\nACTIVATIONS = {\n    'ReLU': torch.nn.ReLU(inplace = False),\n    'LeakyReLU': torch.nn.LeakyReLU(inplace = False),\n}",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "ACTIVATIONS",
        "kind": 5,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "peekOfCode": "ACTIVATIONS = {\n    'ReLU': torch.nn.ReLU(inplace = False),\n    'LeakyReLU': torch.nn.LeakyReLU(inplace = False),\n}\ndef get_load_edge_attribute(gnn_layer:str):\n    no_need_edge_attribute = ['GCNConv', 'GATConv', 'RGCNConv', 'RGATConv']\n    need_edge_attribute = []\n    if gnn_layer in no_need_edge_attribute:\n        return False\n    elif gnn_layer in need_edge_attribute:",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_GNN",
        "documentation": {}
    },
    {
        "label": "add_args_shared",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def add_args_shared(parser: ArgumentParser):\n    parser.add_argument(\n        \"--wandb_mode\",\n        type=str,\n        default=None,\n        help=\"wandb mode. For example `disabled` to disable wandb, which can be useful for debugging.\",\n    )\n    parser.add_argument(\n        \"--kg\",\n        type=str,",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "add_args",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def add_args(parser: ArgumentParser):\n    parser.add_argument(\n        \"--params_to_train\",\n        type=str,\n        default=\"all\",\n        help=\"which parameters to train. 'all' means all parameters. 'head' means only the parameters that are added on top of the pretrained model.\",\n    )\n    parser.add_argument(\n        \"--graph_representation\",\n        type=str,",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_args",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def get_args(parser: ArgumentParser):\n    args = parser.parse_args()\n    if args.reload_data is None:\n        if args.graph_representation in [\"set\"]:\n            args.reload_data = True\n        elif args.graph_representation in [\"lGLM\", \"gGLM\", \"list\"]:\n            args.reload_data = False\n        else:\n            raise ValueError(f\"unknown graph_representation {args.graph_representation}\")\n    if args.num_additional_buckets is None:",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2bool",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def str2bool(s:str)->bool:\n    # input can also be bool\n    if s in ['True', 'true', '1', True]:\n        return True\n    elif s in ['False', 'false', '0', False]:\n        return False\n    elif s in ['None', None]:\n        return None\n    else:   \n        raise ValueError(f\"unknown boolean value {s}\")",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2int",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def str2int(s:str)->Optional[int]:\n    if s in ['None', None]:\n        return None\n    else:\n        return int(float(s))\ndef str2optimizer(s:str)->optim.Optimizer:\n    if s == \"Adam\":\n        return optim.Adam\n    elif s == \"SGD\":\n        return optim.SGD",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2optimizer",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def str2optimizer(s:str)->optim.Optimizer:\n    if s == \"Adam\":\n        return optim.Adam\n    elif s == \"SGD\":\n        return optim.SGD\n    elif s == \"AdamW\":\n        return optim.AdamW\n    else:\n        raise ValueError(f\"unknown optimizer {s}\")\ndef str2criterion(s:str)->nn.Module:",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2criterion",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def str2criterion(s:str)->nn.Module:\n    if s == \"CrossEntropyLoss\":\n        return nn.CrossEntropyLoss\n    else:\n        raise ValueError(f\"unknown criterion {s}\")\ndef freeze_params(s:str, model:GraphT5Classifier) -> None:\n    \"\"\"\n    :param s: string that specifies which parameters to train\n    :param model: model\n    \"\"\"",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "freeze_params",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def freeze_params(s:str, model:GraphT5Classifier) -> None:\n    \"\"\"\n    :param s: string that specifies which parameters to train\n    :param model: model\n    \"\"\"\n    if s == \"all\":  # all\n        pass\n    elif s == \"head\":  # only head\n        for param in model.t5model.parameters():\n            param.requires_grad = False",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "reset_params",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def reset_params(model:GraphT5Classifier) -> None:\n    for param in model.parameters():\n        nn.init.normal_(param)\n    for module in model.t5model.modules():\n        model.t5model._init_weights(module)\ndef str2logging_level(s:str):\n    if s == \"CRITICAL\":\n        return logging.CRITICAL\n    elif s == \"WARNING\":\n        return logging.WARNING",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2logging_level",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def str2logging_level(s:str):\n    if s == \"CRITICAL\":\n        return logging.CRITICAL\n    elif s == \"WARNING\":\n        return logging.WARNING\n    elif s == \"INFO\":\n        return logging.INFO\n    elif s == \"DEBUG\":\n        return logging.DEBUG\n    else:",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def load_data(kg, dataset_construction, radius, num_masked):\n    splits = ['train', 'dev', 'test']\n    fn_graphs = [Path(f\"data/knowledgegraph/{kg}/relation_subgraphs_{dataset_construction}/num_neighbors=[1,2,2,2,2]/num_masked={num_masked}/radius={radius}/{split}_graphs.jsonl\") for split in splits]\n    fn_labels = [Path(f\"data/knowledgegraph/{kg}/relation_subgraphs_{dataset_construction}/num_neighbors=[1,2,2,2,2]/num_masked={num_masked}/radius={radius}/{split}_labels.jsonl\") for split in splits]\n    fn_label2index = Path(f\"data/knowledgegraph/{kg}/relation_subgraphs_{dataset_construction}/num_neighbors=[1,2,2,2,2]/label2index.json\")\n    graphs = {split: [Graph(json.loads(l)) for l in tqdm(fn.open('r'))] for split, fn in zip(splits, fn_graphs)}\n    labels = {split: fn.open('r').readlines() for split, fn in zip(splits, fn_labels)}\n    for split in splits:\n        labels[split] = [l.strip() for l in labels[split] if l.strip()]\n    label_to_index = json.load(fn_label2index.open('r'))",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "data_to_dataT5",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def data_to_dataT5(graph:Graph, tokenizer:T5Tokenizer, label:str, label_to_index:dict, graph_representation:str, eos:str):\n    \"\"\"\n    :param graph: graph to convert\n    :param tokenizer: tokenizer of model\n    :param label: label of the relation\n    :param label_to_index: mapping from label to index\n    :param graph_representation: how to represent the graph. \n    :param eos: end-of-sequence token. Can be `False` for not using an eos token. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph, with a relative position of positive infinity (from node to eos) or negative infinity (from eos to node). `unidirectional` means that the eos token is connected to every node in the graph with a relative position of positive infinity (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which preserves locality when using the local GLM\n    \"\"\"\n    if graph_representation == 'lGLM':",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_batch",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def get_batch(data_instances:List[Data], pad_token_id:int, device:str):\n    \"\"\"\n    can be implemented more efficiently with nested tensors, but they are currently unstable\n    \"\"\"\n    max_seq_len = max([data.input_ids.shape[1] for data in data_instances])\n    if data_instances[0].relative_position is None:\n        assert data_instances[0].sparsity_mask is None\n        assert data_instances[0].use_additional_bucket is None\n        is_sequence_transformer = True\n    else:",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "chunker",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def chunker(data_list:List[Data], batch_size:int):\n    \"\"\"\n    returns a generator that yields batches of size batch_size\n    \"\"\"\n    return (data_list[pos:pos + batch_size] for pos in range(0, len(data_list), batch_size))\ndef get_accuracy(preds:torch.Tensor, label:torch.Tensor):\n    \"\"\"\n    :param preds: shape (batch_size, num_classes)\n    :param label: shape (batch_size)\n    \"\"\"",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_accuracy",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def get_accuracy(preds:torch.Tensor, label:torch.Tensor):\n    \"\"\"\n    :param preds: shape (batch_size, num_classes)\n    :param label: shape (batch_size)\n    \"\"\"\n    return (preds.argmax(dim=1) == label).sum().item() / len(label) * 100\ndef run_eval_epoch(model:GraphT5Classifier, data:List[Data], criterion:nn.Module, batch_size:int, device:str):\n    with torch.no_grad():\n        losses = []\n        accuracies = []",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "run_eval_epoch",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def run_eval_epoch(model:GraphT5Classifier, data:List[Data], criterion:nn.Module, batch_size:int, device:str):\n    with torch.no_grad():\n        losses = []\n        accuracies = []\n        weights = []\n        for data_instances in chunker(data, batch_size):\n            # create batch\n            logging.debug(\"get batch\")\n            input_ids, relative_position, sparsity_mask, use_additional_bucket, indices, label = get_batch(data_instances, pad_token_id=model.tokenizer.pad_token_id, device=device)\n            logging.debug(\"forward\")",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "run_train_epoch",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def run_train_epoch(model:GraphT5Classifier, data:List[Data], criterion:nn.Module, optimizer:torch.optim.Optimizer, batch_size:int, gradient_accumulation_steps:int, device:str):\n    losses = []\n    accuracies = []\n    weights = []\n    optimizer.zero_grad()\n    random.shuffle(data)\n    for i, data_instances in tqdm(enumerate(chunker(data, batch_size)), total=len(data)//batch_size):\n        # create batch\n        input_ids, relative_position, sparsity_mask, use_additional_bucket, indices, label = get_batch(data_instances, pad_token_id=model.tokenizer.pad_token_id, device=device)\n        logits = model.forward(",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "peekOfCode": "def main(args):\n    if not args.device.startswith('cuda'):\n        logging.warning(f'using CPU {args.device}, training might be slow.')\n    else:\n        logging.info(f'using GPU {args.device}')\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    if args.device.startswith('cuda'):\n        torch.cuda.manual_seed(args.seed)\n    else:",
        "detail": "GraphLanguageModels.experiments.encoder.relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "add_args_shared",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "peekOfCode": "def add_args_shared(parser: ArgumentParser):\n    parser.add_argument(\n        \"--wandb_mode\",\n        type=str,\n        default=None,\n        help=\"wandb mode. For example `disabled` to disable wandb, which can be useful for debugging.\",\n    )\n    parser.add_argument(\n        \"--modelsize\",\n        type=str,",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "documentation": {}
    },
    {
        "label": "add_args",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "peekOfCode": "def add_args(parser: ArgumentParser):\n    parser.add_argument(\n        \"--params_to_train\",\n        type=str,\n        default=\"all\",\n        help=\"which parameters to train. 'all' means all parameters. 'head' means only the parameters that are added on top of the pretrained model.\",\n    )\n    parser.add_argument(\n        \"--graph_representation\",\n        type=str,",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "documentation": {}
    },
    {
        "label": "get_args",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "peekOfCode": "def get_args(parser: ArgumentParser):\n    args = parser.parse_args()\n    if args.num_additional_buckets is None:\n        args.num_additional_buckets = 0\n        if args.graph_representation in [\"set\", \"list\",\"lGLM\"]:\n            args.num_additional_buckets += 0\n        elif args.graph_representation in [\"gGLM\"]:\n            args.num_additional_buckets += 1\n        else:\n            raise ValueError(f\"unknown graph_representation {args.graph_representation}\")",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "peekOfCode": "def main(args):\n    if not args.device.startswith('cuda'):\n        logging.warning(f'using CPU {args.device}, training might be slow.')\n    else:\n        logging.info(f'using GPU {args.device}')\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    if args.device.startswith('cuda'):\n        torch.cuda.manual_seed(args.seed)\n    else:",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.evaluate_LM",
        "documentation": {}
    },
    {
        "label": "OpenData",
        "kind": 6,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "class OpenData():\n    def __init__(self, use_graph:bool, entailed_triplets_only:bool):\n        use_graph_name = '' if use_graph else '-triplet_only'\n        entailed_triplets_only_name = '-text_entailed_only' if entailed_triplets_only else ''\n        self.fn = f'data/rebel_dataset/rebel{use_graph_name}{entailed_triplets_only_name}.hdf5'\n    def __enter__(self):\n        self.f = h5py.File(self.fn, 'r')\n        return self.f\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.f.close()",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "add_args_shared",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def add_args_shared(parser: ArgumentParser):\n    parser.add_argument(\n        \"--wandb_mode\",\n        type=str,\n        default=None,\n        help=\"wandb mode. For example `disabled` to disable wandb, which can be useful for debugging.\",\n    )\n    parser.add_argument(\n        \"--modelsize\",\n        type=str,",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "add_args",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def add_args(parser: ArgumentParser):\n    parser.add_argument(\n        \"--params_to_train\",\n        type=str,\n        default=\"all\",\n        help=\"which parameters to train. 'all' means all parameters. 'head' means only the parameters that are added on top of the pretrained model.\",\n    )\n    parser.add_argument(\n        \"--graph_representation\",\n        type=str,",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_args",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def get_args(parser: ArgumentParser):\n    args = parser.parse_args()\n    if args.num_additional_buckets is None:\n        args.num_additional_buckets = 0\n        if args.graph_representation in [\"set\", \"list\",\"lGLM\"]:\n            args.num_additional_buckets += 0\n        elif args.graph_representation in [\"gGLM\"]:\n            args.num_additional_buckets += 1\n        else:\n            raise ValueError(f\"unknown graph_representation {args.graph_representation}\")",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2bool",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def str2bool(s:str)->bool:\n    # input can also be bool\n    if s in ['True', 'true', '1', True]:\n        return True\n    elif s in ['False', 'false', '0', False]:\n        return False\n    elif s in ['None', None]:\n        return None\n    else:   \n        raise ValueError(f\"unknown boolean value {s}\")",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2path",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def str2path(s:str)->Optional[Path]:\n    if s in ['None', None]:\n        return None\n    else:\n        return Path(s)\ndef str2int(s:str)->Optional[int]:\n    if s in ['None', None]:\n        return None\n    else:\n        return int(float(s))",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2int",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def str2int(s:str)->Optional[int]:\n    if s in ['None', None]:\n        return None\n    else:\n        return int(float(s))\ndef str2optimizer(s:str)->optim.Optimizer:\n    if s == \"Adam\":\n        return optim.Adam\n    elif s == \"SGD\":\n        return optim.SGD",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2optimizer",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def str2optimizer(s:str)->optim.Optimizer:\n    if s == \"Adam\":\n        return optim.Adam\n    elif s == \"SGD\":\n        return optim.SGD\n    elif s == \"AdamW\":\n        return optim.AdamW\n    else:\n        raise ValueError(f\"unknown optimizer {s}\")\ndef str2criterion(s:str)->nn.Module:",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2criterion",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def str2criterion(s:str)->nn.Module:\n    if s == \"CrossEntropyLoss\":\n        return nn.CrossEntropyLoss\n    else:\n        raise ValueError(f\"unknown criterion {s}\")\ndef freeze_params(s:str, model:GraphT5Classifier) -> None:\n    \"\"\"\n    :param s: string that specifies which parameters to train\n    :param model: model\n    \"\"\"",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "freeze_params",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def freeze_params(s:str, model:GraphT5Classifier) -> None:\n    \"\"\"\n    :param s: string that specifies which parameters to train\n    :param model: model\n    \"\"\"\n    if s == \"all\":  # all\n        pass\n    elif s == \"head\":  # only head\n        for param in model.t5model.parameters():\n            param.requires_grad = False",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "reset_params",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def reset_params(model:GraphT5Classifier) -> None:\n    for param in model.parameters():\n        nn.init.normal_(param)\n    for module in model.t5model.modules():\n        model.t5model._init_weights(module)\ndef str2logging_level(s:str):\n    if s == \"CRITICAL\":\n        return logging.CRITICAL\n    elif s == \"WARNING\":\n        return logging.WARNING",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "str2logging_level",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def str2logging_level(s:str):\n    if s == \"CRITICAL\":\n        return logging.CRITICAL\n    elif s == \"WARNING\":\n        return logging.WARNING\n    elif s == \"INFO\":\n        return logging.INFO\n    elif s == \"DEBUG\":\n        return logging.DEBUG\n    else:",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "data_to_dataT5",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def data_to_dataT5(graph:Graph, text:str, mask_origin:str, tokenizer:T5Tokenizer, label:int, graph_representation:str, eos:str, use_text:str):\n    \"\"\"\n    :param graph: graph to convert\n    :param text: text to convert\n    :param mask_origin: whether the mask is entailed by text (--> text) or not (--> graph)\n    :param tokenizer: tokenizer of model\n    :param label: label of the relation\n    :param graph_representation: how to represent the graph. \n    :param eos: end-of-sequence token. Can be `False` for not using an eos token. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph, with a relative position of positive infinity (from node to eos) or negative infinity (from eos to node). `unidirectional` means that the eos token is connected to every node in the graph with a relative position of positive infinity (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which preserves locality when using the local GLM\n    :param use_text: whether and how to use text as input. Can be `False` for not using text, `FullyConnected` having a full attention matrix with T2G and G2T attention.",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_data_instances",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def get_data_instances(data:OpenData, split:str, data_indices:List[int], tokenizer:T5Tokenizer, graph_representation:str, eos:str, use_text:str) -> List[Data]:\n    \"\"\"\n    :param data: data\n    :param split: split of data\n    :param data_indices: indices of data instances to get\n    :param tokenizer: tokenizer of model\n    :param graph_representation: how to represent the graph.\n    :param eos: how to handle end of sentence token\n    :param use_text: how to handle text\n    \"\"\"",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_batch",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def get_batch(data:OpenData, split:str, data_indices:List[int], device:str, tokenizer:T5Tokenizer, graph_representation:str, eos:str, use_text:str, max_seq_len:int, predict_source:bool, source_to_index:Dict[str,int]):\n    \"\"\"\n    can be implemented more efficiently with nested tensors, but they are currently unstable\n    \"\"\"\n    data_instances = get_data_instances(data=data, split=split, data_indices=data_indices, tokenizer=tokenizer, graph_representation=graph_representation, eos=eos, use_text=use_text)\n    current_max_seq_len = max([data.input_ids.shape[1] for data in data_instances])\n    max_seq_len = min(max_seq_len, current_max_seq_len)\n    if data_instances[0].relative_position is None:\n        assert data_instances[0].sparsity_mask is None\n        assert data_instances[0].use_additional_bucket is None",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "chunker",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def chunker(data_list:list, batch_size:int):\n    \"\"\"\n    returns a generator that yields batches of size batch_size\n    \"\"\"\n    return (data_list[pos:pos + batch_size] for pos in range(0, len(data_list), batch_size))\ndef get_accuracy(preds:torch.Tensor, label:torch.Tensor):\n    \"\"\"\n    :param preds: shape (batch_size, num_classes)\n    :param label: shape (batch_size)\n    \"\"\"",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_accuracy",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def get_accuracy(preds:torch.Tensor, label:torch.Tensor):\n    \"\"\"\n    :param preds: shape (batch_size, num_classes)\n    :param label: shape (batch_size)\n    \"\"\"\n    return (preds.argmax(dim=1) == label).sum().item() / len(label) * 100\ndef get_preds_and_rank(preds:torch.Tensor, labels:torch.Tensor):\n    \"\"\"\n    :param preds: shape (batch_size, num_classes)\n    :param label: shape (batch_size)",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_preds_and_rank",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def get_preds_and_rank(preds:torch.Tensor, labels:torch.Tensor):\n    \"\"\"\n    :param preds: shape (batch_size, num_classes)\n    :param label: shape (batch_size)\n    :return pred_classes: shape (batch_size). Predicted class for each instance.\n    :return ranks: shape (batch_size). Rank of the correct class. 0 is the best rank.\n    \"\"\"\n    pred_classes = preds.argmax(dim=1)\n    # ranks = preds > preds[labels]\n    # get logit for correct class for each instacne",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "mrr_score",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def mrr_score(ranks:np.ndarray):\n    return np.average(1 / (1 + ranks))\ndef accuracy_score(pred_classes:np.ndarray, labels:np.ndarray):\n    return (pred_classes == labels).mean()\ndef timeit(func):\n    @wraps(func)\n    def timeit_wrapper(*args, **kwargs):\n        start_time = time()\n        result = func(*args, **kwargs)\n        end_time = time()",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def accuracy_score(pred_classes:np.ndarray, labels:np.ndarray):\n    return (pred_classes == labels).mean()\ndef timeit(func):\n    @wraps(func)\n    def timeit_wrapper(*args, **kwargs):\n        start_time = time()\n        result = func(*args, **kwargs)\n        end_time = time()\n        total_time = end_time - start_time\n        logging.info(f'Function {func.__name__} took {total_time:.2f} seconds')",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "timeit",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def timeit(func):\n    @wraps(func)\n    def timeit_wrapper(*args, **kwargs):\n        start_time = time()\n        result = func(*args, **kwargs)\n        end_time = time()\n        total_time = end_time - start_time\n        logging.info(f'Function {func.__name__} took {total_time:.2f} seconds')\n        return result\n    return timeit_wrapper",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "get_metrics",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def get_metrics(pred_classes:List[int], ranks:List[int], labels:List[int], entailed_by_texts:List[bool]):\n    pred_classes = np.array(pred_classes)\n    ranks = np.array(ranks)\n    labels = np.array(labels)\n    entailed_by_texts = np.array(entailed_by_texts)\n    micro_f1 = f1_score(y_true=labels, y_pred=pred_classes, average='micro')\n    macro_f1 = f1_score(y_true=labels, y_pred=pred_classes, average='macro')\n    mrr = mrr_score(ranks)\n    accuracy = accuracy_score(pred_classes, labels)\n    text_pred_classes = pred_classes[entailed_by_texts]",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "run_eval_epoch",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def run_eval_epoch(model:GraphT5Classifier, data:OpenData, batch_size:int, device:str, split:str, graph_representation:str, eos_usage:str, use_text:str, max_seq_len:int, predict_source:bool, source_to_index:Dict[str,int]):\n    with torch.no_grad():\n        # losses = []\n        labels = []\n        pred_classes = []\n        ranks = []  # ranks of the correct class. 0 is the highest rank.\n        entailed_by_texts = []\n        if predict_source:\n            label_sources = []\n            pred_sources = []",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "run_train_epoch",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def run_train_epoch(model:GraphT5Classifier, data:OpenData, data_indicess:List[int], criterion:nn.Module, optimizer:torch.optim.Optimizer, batch_size:int, gradient_accumulation_steps:int, device:str, split:str, graph_representation:str, eos_usage:str, use_text:str, predict_source:bool, source_to_index:Dict[str,int], save_model_after_seen_instances:List[int], save_model_dir:Optional[Path], num_seen_instances:int, stop_training_after_seen_instances:bool):\n    losses = []\n    accuracies = []\n    weights = []\n    accuracies_source = []\n    optimizer.zero_grad()\n    random.shuffle(data_indicess)\n    for i, data_indices in tqdm(enumerate(chunker(data_indicess, batch_size)), total=len(data_indicess)//batch_size):\n        # create batch\n        input_ids, relative_position, sparsity_mask, use_additional_bucket, indices, label, entailed_by_text, source_label = get_batch(data=data, split=split, data_indices=data_indices, device=device, tokenizer=model.tokenizer, graph_representation=graph_representation, eos=eos_usage, use_text=use_text, max_seq_len=args.max_seq_len, predict_source=predict_source, source_to_index=source_to_index)",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "description": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "peekOfCode": "def main(args):\n    if args.continue_training:\n        assert args.save_model_dir is not None\n        assert args.save_model_dir.exists()\n    else:\n        if args.save_model_dir is not None:\n            if args.save_model_dir.exists():\n                assert len(list(args.save_model_dir.glob('*'))) == 0, f'{args.save_model_dir} is not empty: {list(args.save_model_dir.glob(\"*\"))}'\n            args.save_model_dir.mkdir(parents=True, exist_ok=True)\n            logging.info(f'saving model to {args.save_model_dir}')",
        "detail": "GraphLanguageModels.experiments.encoder.text_guided_relation_prediction.train_LM",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "peekOfCode": "class T5Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`T5Model`] or a [`TFT5Model`]. It is used to\n    instantiate a T5 model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the T5\n    [t5-small](https://huggingface.co/t5-small) architecture.\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Arguments:\n        vocab_size (`int`, *optional*, defaults to 32128):",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "peekOfCode": "logger = logging.get_logger(__name__)\nT5_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/config.json\",\n    \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/config.json\",\n    \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/config.json\",\n    \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/config.json\",\n    \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/config.json\",\n}\nclass T5Config(PretrainedConfig):\n    r\"\"\"",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "documentation": {}
    },
    {
        "label": "T5_PRETRAINED_CONFIG_ARCHIVE_MAP",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "peekOfCode": "T5_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/config.json\",\n    \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/config.json\",\n    \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/config.json\",\n    \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/config.json\",\n    \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/config.json\",\n}\nclass T5Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`T5Model`] or a [`TFT5Model`]. It is used to",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.configuration_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerNorm",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5DenseActDense",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5DenseActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n    def forward(self, hidden_states):\n        hidden_states = self.wi(hidden_states)\n        hidden_states = self.act(hidden_states)",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5DenseGatedActDense",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5DenseGatedActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n    def forward(self, hidden_states):\n        hidden_gelu = self.act(self.wi_0(hidden_states))",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerFF",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5LayerFF(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        if config.is_gated_act:\n            self.DenseReluDense = T5DenseGatedActDense(config)\n        else:\n            self.DenseReluDense = T5DenseActDense(config)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(self, hidden_states):",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Attention",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5Attention(nn.Module):\n    def __init__(self, config: T5Config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.relative_attention_num_additional_buckets = (\n            config.relative_attention_num_additional_buckets\n            if \"relative_attention_num_additional_buckets\" in config.__dict__.keys()\n            else 0",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerSelfAttention",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5LayerSelfAttention(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.SelfAttention = T5Attention(\n            config, has_relative_attention_bias=has_relative_attention_bias\n        )\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(\n        self,",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5LayerCrossAttention",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5LayerCrossAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n    def forward(\n        self,\n        hidden_states,\n        key_value_states,",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Block",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5Block(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.layer = nn.ModuleList()\n        self.layer.append(\n            T5LayerSelfAttention(\n                config, has_relative_attention_bias=has_relative_attention_bias\n            )\n        )",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5PreTrainedModel",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5PreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n    config_class = T5Config\n    load_tf_weights = load_tf_weights_in_t5\n    base_model_prefix = \"transformer\"\n    is_parallelizable = True\n    supports_gradient_checkpointing = True",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Stack",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5Stack(T5PreTrainedModel):\n    def __init__(self, config, embed_tokens=None):\n        super().__init__(config)\n        self.embed_tokens = embed_tokens\n        self.is_decoder = config.is_decoder\n        self.block = nn.ModuleList(\n            [\n                T5Block(config, has_relative_attention_bias=bool(i == 0))\n                for i in range(config.num_layers)\n            ]",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Model",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5Model(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n    def __init__(self, config: T5Config):\n        super().__init__(config)",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5ForConditionalGeneration(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n        r\"lm_head.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n    def __init__(self, config: T5Config):",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "class T5EncoderModel(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"encoder.embed_tokens.weight\"]\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n        encoder_config = copy.deepcopy(config)\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n        # Initialize weights and apply final processing",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "load_tf_weights_in_t5",
        "kind": 2,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "def load_tf_weights_in_t5(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    raise NotImplementedError(\"might need adjustments for GLM\")\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "logger = transformers_logging.get_logger(__name__)\n_CONFIG_FOR_DOC = \"T5Config\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "_CONFIG_FOR_DOC",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "_CONFIG_FOR_DOC = \"T5Config\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "_CHECKPOINT_FOR_DOC",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "_CHECKPOINT_FOR_DOC = \"t5-small\"\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_PRETRAINED_MODEL_ARCHIVE_LIST",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "T5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",\n    \"t5-11b\",\n    # See all T5 models at https://huggingface.co/models?filter=t5\n]\n####################################################\n# This is a conversion method from TF 1.0 to PyTorch",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "PARALLELIZE_DOCSTRING",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "PARALLELIZE_DOCSTRING = r\"\"\"\n    This is an experimental feature and is a subject to change at a moment's notice.\n    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n    it will evenly distribute blocks across all devices.\n    Args:\n        device_map (`Dict[int, list]`, optional, defaults to None):\n            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n            have fewer attention modules mapped to it than other devices. For reference, the t5 models have the\n            following number of attention modules:",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "DEPARALLELIZE_DOCSTRING",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "DEPARALLELIZE_DOCSTRING = r\"\"\"\n    Moves the model to cpu from a model parallel state.\n    Example:\n    ```python\n    # On a 4 GPU machine with t5-3b:\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n    device_map = {\n        0: [0, 1, 2],\n        1: [3, 4, 5, 6, 7, 8, 9],\n        2: [10, 11, 12, 13, 14, 15, 16],",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_START_DOCSTRING",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "T5_START_DOCSTRING = r\"\"\"\n    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\n    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a\n    text-to-text denoising generative setting.\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "T5_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n            [What are input IDs?](../glossary#input-ids)\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5_ENCODER_INPUTS_DOCSTRING",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "T5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "__HEAD_MASK_WARNING_MSG",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "peekOfCode": "__HEAD_MASK_WARNING_MSG = \"\"\"\nThe input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\nIf you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\nnum_heads)`.\n\"\"\"\n@add_start_docstrings(\n    \"The bare T5 Model transformer outputting raw hidden-states without any specific head on top.\",\n    T5_START_DOCSTRING,\n)",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.modeling_t5",
        "documentation": {}
    },
    {
        "label": "T5Tokenizer",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "peekOfCode": "class T5Tokenizer(PreTrainedTokenizer):\n    \"\"\"\n    Construct a T5 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).\n    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n    this superclass for more information regarding those methods.\n    Args:\n        vocab_file (`str`):\n            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n            contains the vocabulary necessary to instantiate a tokenizer.\n        eos_token (`str`, *optional*, defaults to `\"</s>\"`):",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "peekOfCode": "logger = logging.get_logger(__name__)\nVOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/spiece.model\",\n        \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/spiece.model\",\n        \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/spiece.model\",\n        \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/spiece.model\",\n        \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/spiece.model\",\n    }",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "documentation": {}
    },
    {
        "label": "VOCAB_FILES_NAMES",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "peekOfCode": "VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/spiece.model\",\n        \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/spiece.model\",\n        \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/spiece.model\",\n        \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/spiece.model\",\n        \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/spiece.model\",\n    }\n}",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "documentation": {}
    },
    {
        "label": "PRETRAINED_VOCAB_FILES_MAP",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "peekOfCode": "PRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/spiece.model\",\n        \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/spiece.model\",\n        \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/spiece.model\",\n        \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/spiece.model\",\n        \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/spiece.model\",\n    }\n}\n# TODO(PVP) - this should be removed in Transformers v5",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "documentation": {}
    },
    {
        "label": "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "peekOfCode": "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \"t5-small\": 512,\n    \"t5-base\": 512,\n    \"t5-large\": 512,\n    \"t5-3b\": 512,\n    \"t5-11b\": 512,\n}\nclass T5Tokenizer(PreTrainedTokenizer):\n    \"\"\"\n    Construct a T5 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5",
        "documentation": {}
    },
    {
        "label": "T5TokenizerFast",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "peekOfCode": "class T5TokenizerFast(PreTrainedTokenizerFast):\n    \"\"\"\n    Construct a \"fast\" T5 tokenizer (backed by HuggingFace's *tokenizers* library). Based on\n    [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).\n    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n    refer to this superclass for more information regarding those methods.\n    Args:\n        vocab_file (`str`):\n            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n            contains the vocabulary necessary to instantiate a tokenizer.",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "peekOfCode": "logger = logging.get_logger(__name__)\nVOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/spiece.model\",\n        \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/spiece.model\",\n        \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/spiece.model\",\n        \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/spiece.model\",\n        \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/spiece.model\",\n    },",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "documentation": {}
    },
    {
        "label": "VOCAB_FILES_NAMES",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "peekOfCode": "VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/spiece.model\",\n        \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/spiece.model\",\n        \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/spiece.model\",\n        \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/spiece.model\",\n        \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/spiece.model\",\n    },\n    \"tokenizer_file\": {",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "documentation": {}
    },
    {
        "label": "PRETRAINED_VOCAB_FILES_MAP",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "peekOfCode": "PRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/spiece.model\",\n        \"t5-base\": \"https://huggingface.co/t5-base/resolve/main/spiece.model\",\n        \"t5-large\": \"https://huggingface.co/t5-large/resolve/main/spiece.model\",\n        \"t5-3b\": \"https://huggingface.co/t5-3b/resolve/main/spiece.model\",\n        \"t5-11b\": \"https://huggingface.co/t5-11b/resolve/main/spiece.model\",\n    },\n    \"tokenizer_file\": {\n        \"t5-small\": \"https://huggingface.co/t5-small/resolve/main/tokenizer.json\",",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "documentation": {}
    },
    {
        "label": "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES",
        "kind": 5,
        "importPath": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "description": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "peekOfCode": "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \"t5-small\": 512,\n    \"t5-base\": 512,\n    \"t5-large\": 512,\n    \"t5-3b\": 512,\n    \"t5-11b\": 512,\n}\nclass T5TokenizerFast(PreTrainedTokenizerFast):\n    \"\"\"\n    Construct a \"fast\" T5 tokenizer (backed by HuggingFace's *tokenizers* library). Based on",
        "detail": "GraphLanguageModels.models.graph_T5.graph_t5.tokenization_t5_fast",
        "documentation": {}
    },
    {
        "label": "GraphT5ForConditionalGeneration",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.autoregressive_GLM",
        "description": "GraphLanguageModels.models.graph_T5.autoregressive_GLM",
        "peekOfCode": "class GraphT5ForConditionalGeneration(PreTrainedModel):\n    config_class = T5Config\n    def __init__(self, config: T5Config, encoder_model_size: str = \"t5-large\"):\n        super().__init__(config=config)\n        self.config = config\n        self.encoder_model_size = encoder_model_size\n        self.tokenizer = T5Tokenizer.from_pretrained(\n            self.config.modelsize, model_max_length=self.config.model_max_length\n        )\n        if \"flan\" in self.config.modelsize:",
        "detail": "GraphLanguageModels.models.graph_T5.autoregressive_GLM",
        "documentation": {}
    },
    {
        "label": "GraphT5Classifier",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.classifier",
        "description": "GraphLanguageModels.models.graph_T5.classifier",
        "peekOfCode": "class GraphT5Classifier(PreTrainedModel):\n    config_class = T5Config\n    def __init__(\n        self,\n        config: T5Config,\n    ):\n        super().__init__(config=config)\n        self.config = config\n        self.tokenizer = T5Tokenizer.from_pretrained(self.config.modelsize, model_max_length=self.config.model_max_length)\n        self.t5model = T5EncoderModel.from_pretrained(self.config.modelsize, config=config, ignore_mismatched_sizes=True)  # when intialiting the model with .from_pretrained, the weights are loaded from the pretrained model, so the t5 parameters are not actually used in that case. Loading them here is unnecessary overhead. ",
        "detail": "GraphLanguageModels.models.graph_T5.classifier",
        "documentation": {}
    },
    {
        "label": "DualGraphT5Classifier",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.classifier",
        "description": "GraphLanguageModels.models.graph_T5.classifier",
        "peekOfCode": "class DualGraphT5Classifier(PreTrainedModel):\n    \"\"\"\n    Same as GraphT5Classifier, but with two classification heads\n    \"\"\"\n    config_class = T5Config\n    def __init__(\n        self,\n        config: T5Config,\n    ):\n        super().__init__(config=config)",
        "detail": "GraphLanguageModels.models.graph_T5.classifier",
        "documentation": {}
    },
    {
        "label": "Graph",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "class Graph:\n    \"\"\"\n    A graph class.\n    :param g: A list of tuples, where each tuple is a triple (head, r, tail).\n    \"\"\"\n    def __init__(self, g: List[Tuple[str, str, str]] = []):\n        self.g = g\n        self.concepts = self.get_concepts()  # list of all concepts in the graph\n        self.relations = self.get_relations()  # list of all relations in the graph\n        self.relations_multiple = (",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Data",
        "kind": 6,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "class Data(Namespace):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.__dict__.update(kwargs)\ndef get_dummy_graph(num_triplets: int = 3) -> Graph:\n    g = [\n        (\"dog\", \"IsA\", \"animal\"),\n        (\"cat\", \"IsA\", \"animal\"),\n        (\"black poodle\", \"IsA\", \"dog\"),\n        (\"black cat\", \"IsA\", \"cat\"),",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_dummy_graph",
        "kind": 2,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "def get_dummy_graph(num_triplets: int = 3) -> Graph:\n    g = [\n        (\"dog\", \"IsA\", \"animal\"),\n        (\"cat\", \"IsA\", \"animal\"),\n        (\"black poodle\", \"IsA\", \"dog\"),\n        (\"black cat\", \"IsA\", \"cat\"),\n    ]\n    assert num_triplets <= 4, \"num_triplets must be <= 4\"\n    g = g[:num_triplets]\n    g = Graph(g)",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_r2nl_dict",
        "kind": 2,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "def get_r2nl_dict() -> dict:\n    relations = {\n        # ConceptNet\n        \"/r/RelatedTo\": \"is related to\",\n        \"/r/IsA\": \"is a\",\n        \"/r/FormOf\": \"is a form of\",\n        \"/r/CapableOf\": \"is capable of\",\n        \"/r/MotivatedByGoal\": \"is motivated by\",\n        \"/r/HasContext\": \"has context\",\n        \"/r/HasPrerequisite\": \"has prerequisite\",",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "r2nl",
        "kind": 2,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "def r2nl(r: str) -> str:\n    \"\"\"\n    Convert a relation to a natural language string.\n    \"\"\"\n    r2nl_dict = get_r2nl_dict()  # get_r2nl_dict is cached\n    if r in r2nl_dict.keys():\n        return r2nl_dict[r]\n    else:\n        return r\ndef _get_str2tok(g: Graph, tokenizer: T5Tokenizer) -> dict[str, list[int]]:",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_graphT5",
        "kind": 2,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "def graph_to_graphT5(g: Graph, tokenizer: T5Tokenizer, how: str, eos: str) -> Data:\n    \"\"\"\n    Convert a graph to a graphT5 input.\n    :param g: graph\n    :param tokenizer: tokenizer\n    :param how: how to represent the graph. Can be 'local' or 'global' for lGLM and gGLM respectively.\n    :param eos: end-of-sequence token. Can be `False` for not using an eos token. When using an eos token, there are two ways to use it: `bidirectional` means that the eos token is connected to every other node in the graph, with a relative position of positive infinity (from node to eos) or negative infinity (from eos to node). `unidirectional` means that the eos token is connected to every node in the graph with a relative position of positive infinity (from node to eos), but not the other way around (i.e. no connection from eos to other node). This means, that nodes do not get messages from the eos token, which perceives locality when using the local GLM\n    \"\"\"\n    eos = str(eos)\n    assert eos in [",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "graph_to_set_of_triplets",
        "kind": 2,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "def graph_to_set_of_triplets(\n    g: Graph, tokenizer: T5Tokenizer, order: str = \"random\"\n) -> Data:\n    \"\"\"\n    Convert graph to a T5 input where graph as represented as a set of triplets. Triplets are separated by </s>.\n    :param g: graph\n    :param tokenizer: tokenizer\n    :param order: order of triplets. Can be 'random' or 'alphabetical'\n    \"\"\"\n    str2tok = _get_str2tok(",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "kind": 2,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "def get_embedding(\n    sequence_embedding: torch.Tensor,\n    indices: Dict[str, List[Tuple[int, int]]],\n    concept: str,\n    embedding_aggregation: str = \"mean\",\n):\n    \"\"\"\n    Returns the embedding of a concept.\n    :param sequence_embedding: the embedding of the whole sequence. shape: (sequence_length, embedding_size)\n    :param indices: dictionary mapping each node to its start-index and end- in the sequence. Keys are nodes, values are lists of tuples (start_index, end_index). The lists have a length of 1 for concepts.",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "add_text_to_graph_data",
        "kind": 2,
        "importPath": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "description": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "peekOfCode": "def add_text_to_graph_data(data, text, tokenizer, use_text):\n    if use_text in {\"False\", \"\", False, None}:\n        return None\n    text_seq = torch.tensor(tokenizer(text, padding=False)[\"input_ids\"]).unsqueeze(0)\n    new_input_ids = torch.cat([data.input_ids, text_seq], dim=1)\n    old_seq_len = data.input_ids.shape[1]\n    text_seq_len = text_seq.shape[1]\n    new_seq_len = new_input_ids.shape[1]\n    new_is_graph = torch.zeros(size=(1, new_seq_len), dtype=torch.bool)\n    new_is_graph[:, :old_seq_len] = True",
        "detail": "GraphLanguageModels.models.graph_T5.wrapper_functions",
        "documentation": {}
    },
    {
        "label": "Args",
        "kind": 6,
        "importPath": "GraphLanguageModels.preprocessing.get_node_embeddings",
        "description": "GraphLanguageModels.preprocessing.get_node_embeddings",
        "peekOfCode": "class Args(Namespace):\n    def __init__(self):\n        super().__init__()\n        self.kg = 'conceptnet'\n        self.kg_in_fn = Path(f'data/knowledgegraph/{self.kg}/graph.pkl')\n        self.out_fn = Path(f'data/knowledgegraph/{self.kg}/node_embeddings.json')\n        self.model_id = 'all-mpnet-base-v2'  # model id of SBERT\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndef main(args):\n    # load kg",
        "detail": "GraphLanguageModels.preprocessing.get_node_embeddings",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.get_node_embeddings",
        "description": "GraphLanguageModels.preprocessing.get_node_embeddings",
        "peekOfCode": "def main(args):\n    # load kg\n    g = ig.read(args.kg_in_fn, format=\"pickle\")\n    node_names = g.vs['name']\n    assert len(node_names) == len(set(node_names))\n    del g\n    node_names.sort()\n    # load SBERT\n    model = SentenceTransformer(args.model_id, device=args.device)\n    # get node embeddings",
        "detail": "GraphLanguageModels.preprocessing.get_node_embeddings",
        "documentation": {}
    },
    {
        "label": "Args",
        "kind": 6,
        "importPath": "GraphLanguageModels.preprocessing.mask_subgraph",
        "description": "GraphLanguageModels.preprocessing.mask_subgraph",
        "peekOfCode": "class Args(Namespace):\n    def __init__(self, num_masked: int, radius: int):\n        super().__init__()\n        self.num_masked = num_masked\n        self.radius = radius\n        self.in_dir = Path(\n            f\"data/knowledgegraph/conceptnet/concept_subgraphs_semantic_label-by-degree/num_neighbors=[2,2,2,2,2]/num_masked=0/radius={self.radius}\"\n        )\n        self.out_dir = Path(\n            f\"data/knowledgegraph/conceptnet/concept_subgraphs_semantic_label-by-degree/num_neighbors=[2,2,2,2,2]/num_masked={self.num_masked}/radius={self.radius}\"",
        "detail": "GraphLanguageModels.preprocessing.mask_subgraph",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.mask_subgraph",
        "description": "GraphLanguageModels.preprocessing.mask_subgraph",
        "peekOfCode": "def load_data(dir: str):\n    splits = [\"train\", \"dev\", \"test\"]\n    fn_graphs = [Path(dir, f\"{split}_graphs.jsonl\") for split in splits]\n    fn_labels = [Path(dir, f\"{split}_labels.jsonl\") for split in splits]\n    graphs = {\n        split: [Graph(json.loads(l)) for l in tqdm(fn.open(\"r\"))]\n        for split, fn in zip(splits, fn_graphs)\n    }\n    labels = {split: fn.open(\"r\").read() for split, fn in zip(splits, fn_labels)}\n    return graphs, labels",
        "detail": "GraphLanguageModels.preprocessing.mask_subgraph",
        "documentation": {}
    },
    {
        "label": "save_data",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.mask_subgraph",
        "description": "GraphLanguageModels.preprocessing.mask_subgraph",
        "peekOfCode": "def save_data(graphs: dict, labels: dict, out_dir: str):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    for split in graphs.keys():\n        with Path(out_dir, f\"{split}_graphs.jsonl\").open(\"w\") as f:\n            for graph in graphs[split]:\n                f.write(json.dumps(graph))\n                f.write(\"\\n\")\n        with Path(out_dir, f\"{split}_labels.jsonl\").open(\"w\") as f:\n            f.write(labels[split])\ndef main(args):",
        "detail": "GraphLanguageModels.preprocessing.mask_subgraph",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.mask_subgraph",
        "description": "GraphLanguageModels.preprocessing.mask_subgraph",
        "peekOfCode": "def main(args):\n    print(\"load data\")\n    all_graphs, all_labels = load_data(args.in_dir)\n    print(\"mask data\")\n    masked_graphs = {\n        split: [] for split in all_graphs.keys()\n    }  # initialize masked graphs\n    for split, graphs in all_graphs.items():\n        for graph in tqdm(graphs):\n            graph.mask_neighbors(args.num_masked)",
        "detail": "GraphLanguageModels.preprocessing.mask_subgraph",
        "documentation": {}
    },
    {
        "label": "Args",
        "kind": 6,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "class Args(Namespace):\n    def __init__(self):\n        super().__init__()\n        self.add_no_relation = True\n        self.no_relation_prob = 0.1\n        self.no_relation_label = 'has no relation to'  # this verbalization of the label is not used in the experiments, because it will always be replaced by a mask token\n        self.in_fns = {\n            split: Path(f'data/rebel_dataset/en_{split}.jsonl')\n            for split in ['train', 'val', 'test']\n            # for split in ['dummy']",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "load_jsonl",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def load_jsonl(fn:Path):\n    return [json.loads(l) for l in tqdm(fn.open('r'))]\ndef save_jsonl(data:list, fn:Path):\n    out_str = '\\n'.join([json.dumps(d) for d in data])\n    fn.open('w').write(out_str)\ndef get_all_relations(data:dict, splits:list)->list[str]:\n    relations = [t['predicate']['surfaceform'] for split in splits for d in data[split] for t in d['triples']]\n    return relations\ndef get_relation_counts(data:dict, splits:list):\n    relations = get_all_relations(data, splits)",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "save_jsonl",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def save_jsonl(data:list, fn:Path):\n    out_str = '\\n'.join([json.dumps(d) for d in data])\n    fn.open('w').write(out_str)\ndef get_all_relations(data:dict, splits:list)->list[str]:\n    relations = [t['predicate']['surfaceform'] for split in splits for d in data[split] for t in d['triples']]\n    return relations\ndef get_relation_counts(data:dict, splits:list):\n    relations = get_all_relations(data, splits)\n    counter = Counter(relations)\n    return counter",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_all_relations",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_all_relations(data:dict, splits:list)->list[str]:\n    relations = [t['predicate']['surfaceform'] for split in splits for d in data[split] for t in d['triples']]\n    return relations\ndef get_relation_counts(data:dict, splits:list):\n    relations = get_all_relations(data, splits)\n    counter = Counter(relations)\n    return counter\ndef get_label_to_index(data:dict, split:str='train', num_labels:int=220, add_no_relation:bool=True, no_relation_label:str='has no relation to')->dict[str, int]:\n    relations_counter = get_relation_counts(data, [split])\n    label_to_index = {label: i for i, (label, _) in enumerate(relations_counter.most_common(num_labels))}",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_relation_counts",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_relation_counts(data:dict, splits:list):\n    relations = get_all_relations(data, splits)\n    counter = Counter(relations)\n    return counter\ndef get_label_to_index(data:dict, split:str='train', num_labels:int=220, add_no_relation:bool=True, no_relation_label:str='has no relation to')->dict[str, int]:\n    relations_counter = get_relation_counts(data, [split])\n    label_to_index = {label: i for i, (label, _) in enumerate(relations_counter.most_common(num_labels))}\n    assert no_relation_label not in label_to_index.keys()\n    if add_no_relation:\n        label_to_index[no_relation_label] = len(label_to_index)",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_label_to_index",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_label_to_index(data:dict, split:str='train', num_labels:int=220, add_no_relation:bool=True, no_relation_label:str='has no relation to')->dict[str, int]:\n    relations_counter = get_relation_counts(data, [split])\n    label_to_index = {label: i for i, (label, _) in enumerate(relations_counter.most_common(num_labels))}\n    assert no_relation_label not in label_to_index.keys()\n    if add_no_relation:\n        label_to_index[no_relation_label] = len(label_to_index)\n    return label_to_index\ndef data_to_surfaceform(data:dict):\n    data = {\n        split: [",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "data_to_surfaceform",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def data_to_surfaceform(data:dict):\n    data = {\n        split: [\n            {\n                'triplets': [[t['subject']['surfaceform'], t['predicate']['surfaceform'], t['object']['surfaceform']] for t in d['triples']], \n                'entities': [e['surfaceform'] for e in d['entities']], \n                'title': d['title'],\n                'text': d['text']\n            } \n            for d in tqdm(data[split])",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "data_to_uri",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def data_to_uri(data:dict):\n    data = {\n        split: [\n            {\n                'triplets': [[t['subject']['uri'], t['predicate']['uri'], t['object']['uri']] for t in d['triples']], \n                'entities': [e['uri'] for e in d['entities']], \n            } \n            for d in tqdm(data[split])\n        ]\n        for split in data.keys()",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "load_wikidata_triplets",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def load_wikidata_triplets(fn)->pd.DataFrame:\n    df = pd.read_csv(fn, sep='\\t', header=None, names=['subject', 'predicate', 'object'])\n    # df['entities'] = df[['subject', 'object']].apply(lambda x: tuple(sorted(x)), axis=1)\n    return df\ndef get_sparql():\n    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n    sparql.setReturnFormat('json')\n    return sparql\ndef get_query(ids:list[str]):\n    ids_str = 'wd:' + ' wd:'.join(ids)",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_sparql",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_sparql():\n    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n    sparql.setReturnFormat('json')\n    return sparql\ndef get_query(ids:list[str]):\n    ids_str = 'wd:' + ' wd:'.join(ids)\n    query = \"\"\"\n    SELECT ?s ?p ?o\n    WHERE {\n    VALUES ?s { <IDS> }",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_query",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_query(ids:list[str]):\n    ids_str = 'wd:' + ' wd:'.join(ids)\n    query = \"\"\"\n    SELECT ?s ?p ?o\n    WHERE {\n    VALUES ?s { <IDS> }\n    VALUES ?o { <IDS> }\n    { ?s ?p ?o }\n    }\n    \"\"\"",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_connecting_triplets",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_connecting_triplets(ids:list[str], sparql:SPARQLWrapper):\n    \"\"\"\n    returns list of triplets that connect two entities in ids. Each triplet is a list with three elements: subject, predicate, object\n    \"\"\"\n    query = get_query(ids)\n    sparql.setQuery(query)\n    results = sparql.query().convert()\n    results = results[\"results\"][\"bindings\"]\n    results = [\n        [",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "remove_external_links",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def remove_external_links(ids:list[str]):\n    ids = [e for e in ids if e.startswith(('P', 'Q', 'L')) and e[1:].isnumeric()]\n    return ids\ndef augment_with_wikidata_triplets(data:dict, sparql:SPARQLWrapper, out_fn:Path):\n    \"\"\"\n    Augment with all triplets in wikidata that directly connect two entities in rebel. Works in-place. \n    \"\"\"\n    if out_fn is None:\n        num_done_already = 0\n    else:",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "augment_with_wikidata_triplets",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def augment_with_wikidata_triplets(data:dict, sparql:SPARQLWrapper, out_fn:Path):\n    \"\"\"\n    Augment with all triplets in wikidata that directly connect two entities in rebel. Works in-place. \n    \"\"\"\n    if out_fn is None:\n        num_done_already = 0\n    else:\n        num_done_already = sum(1 for i in open(out_fn, 'rb')) if out_fn.exists() else 0\n    logging.info(f'num_done_already: {num_done_already}')\n    for d in tqdm(data[num_done_already:], total=len(data), initial=num_done_already):",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_query_property_surfaceform",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_query_property_surfaceform(idx:str):\n    ids_str = f'wd:{idx}'\n    query = \"\"\"\n    SELECT ?p ?pLabel\n    WHERE {\n        VALUES ?p { <IDS> }\n        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n    }\n    \"\"\"\n    query = query.replace('<IDS>', ids_str)",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_surfaceform_of_property",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_surfaceform_of_property(idx:str, sparql:SPARQLWrapper, property_surfaceform:dict, fn_property_surfaceform:Path):\n    if idx in property_surfaceform.keys():\n        return property_surfaceform[idx]\n    logging.debug(f'getting surfaceform of property {idx} from wikidata API')\n    query = get_query_property_surfaceform(idx)\n    sparql.setQuery(query)\n    results = sparql.query().convert()\n    result = results[\"results\"][\"bindings\"][0][\"pLabel\"][\"value\"]\n    property_surfaceform[idx] = result  # modify in-place\n    if fn_property_surfaceform is not None:",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "add_surfaceform_to_predicate",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def add_surfaceform_to_predicate(data:dict, sparql:SPARQLWrapper, fn_property_surfaceform:Path):\n    property_surfaceform = {l.split('\\t')[0]: l.split('\\t')[1].strip() for l in fn_property_surfaceform.open('r') if l.strip()}\n    for d in tqdm(data):\n        del_triplets = []\n        for i, t in enumerate(d['triples']):\n            idx = t['predicate']['uri']\n            if not (idx.startswith('P') and idx[1:].isnumeric()):\n                del_triplets.append(i)\n                continue\n            completed = False",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "fix_annotator",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def fix_annotator(data_augmented:list, data:list):\n    \"\"\"\n    if a triplet occurs in data, then the annotator for that triplet in data_augmented is set to `rebel`. \n    \"\"\"\n    assert len(data_augmented) == len(data), f'{len(data_augmented)} != {len(data)}'\n    for d_aug, d in tqdm(zip(data_augmented, data), total=len(data_augmented)):\n        data_triples = [(t['subject']['uri'], t['predicate']['uri'], t['object']['uri']) for t in d['triples']]\n        for t in d_aug['triples']:\n            if (t['subject']['uri'], t['predicate']['uri'], t['object']['uri']) in data_triples:\n                t['predicate']['annotator'] = 'rebel'",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "get_instance",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def get_instance(d:dict, label_to_index:dict, add_no_relation:bool, no_relation_label:str, no_relation_prob:float):\n    # print(d)\n    try:\n        instance = {\n            'docid': d['docid'],\n            'uri': d['uri'],\n            'title': d['title'],\n            'text': d['text'],\n            'triplets': [[t['subject']['surfaceform'], t['predicate']['surfaceform'], t['object']['surfaceform']] for t in d['triples']],\n            'predicate_annotations': [t['predicate']['annotator'] for t in d['triples']],",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "data_to_hf_dataset",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def data_to_hf_dataset(data:dict, label_to_index:dict, out_fn:Path, add_no_relation:bool, no_relation_label:str, no_relation_prob:float):\n    \"\"\"\n    converts data to HF readable dataset. \n    Can be loaded with\n    from datasets import load_dataset\n    data = load_dataset('json', data_files={split: f'data/rebel_dataset/en_{split}_hf_dataset.jsonl' for split in splits})\n    \"\"\"\n    data = [\n        get_instance(d, label_to_index, add_no_relation, no_relation_label, no_relation_prob)\n        for d in tqdm(data)",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "hf_dataset_to_h5py",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def hf_dataset_to_h5py(in_fns:dict[str,str], out_fn, label_to_index:dict[str,int]):\n    in_fns = {split: str(fn) for split, fn in in_fns.items()}\n    data = load_dataset('json', data_files=in_fns)\n    with h5py.File(out_fn, 'w') as f:\n        for split in data.keys():\n            dst = f.create_dataset(split, (len(data[split]),), dtype=h5py.string_dtype())\n            for i, d in tqdm(enumerate(data[split])):\n                dst[i] = json.dumps(d)\n        f.attrs['num_labels'] = len(label_to_index)\n        f.attrs['label_to_index'] = json.dumps(label_to_index)",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "add_onetriplet_dataset_to_h5py",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def add_onetriplet_dataset_to_h5py(in_fn:str, out_fn:str):\n    \"\"\"\n    In the new file, triplets are removed except for triplet that has the masked relation. Thus, not graph information is available in this setting. \n    \"\"\"\n    with h5py.File(out_fn, 'w') as f_out:\n        with h5py.File(in_fn, 'r') as f_in:\n            for split in f_in.keys():\n                logging.info(f'processing {split}')\n                dst = f_out.create_dataset(split, (len(f_in[split]),), dtype=h5py.string_dtype())\n                for i, d in tqdm(enumerate(f_in[split]), total=len(f_in[split])):",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "text_entailed_only_to_h5py",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def text_entailed_only_to_h5py(in_fn:str, out_fn:str):\n    \"\"\"\n    In the new file, only instances where (i) the masked relation is entailed by the text or (ii) the relation is no-relation are included. \n    \"\"\"\n    with h5py.File(out_fn, 'w') as f_out:\n        with h5py.File(in_fn, 'r') as f_in:\n            for split in f_in.keys():\n                logging.info(f'processing {split}')\n                num_new_instances = sum(1 for d in f_in[split] if json.loads(d)['mask_origin'] in ['text', 'no_relation'])\n                dst = f_out.create_dataset(split, (num_new_instances,), dtype=h5py.string_dtype())",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.rebel",
        "description": "GraphLanguageModels.preprocessing.rebel",
        "peekOfCode": "def main(args):\n    data = None\n    data_augmented = None\n    data_augmented_surfaceform = None\n    if not (args.label_to_index_fn.exists() and args.index_to_label_fn.exists()):\n        if data is None:\n            logging.info('load data')\n            data = {split: load_jsonl(fn) for split, fn in args.in_fns.items()}\n        logging.info('create label_to_index and index_to_label')\n        label_to_index = get_label_to_index(data, split='train', num_labels=220, add_no_relation=args.add_no_relation, no_relation_label=args.no_relation_label)",
        "detail": "GraphLanguageModels.preprocessing.rebel",
        "documentation": {}
    },
    {
        "label": "Args",
        "kind": 6,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "peekOfCode": "class Args(Namespace):\n    def __init__(\n        self, \n        kg: str = \"conceptnet\", \n        radii: List[int] = [1,2],\n        splitsizes_per_relation: Tuple[int,int,int] = (800, 100, 100),\n        skip_relations: Set[str] = set(),\n    ):\n        \"\"\"\n        :param kg: name of the knowledge graph",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "documentation": {}
    },
    {
        "label": "sample_triplets",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "peekOfCode": "def sample_triplets(g:ig.Graph, num_graphs_per_relation:int, relations:List[str], seed:int) -> List[Tuple[str,str]]:\n    \"\"\"\n    Sample triplets from the graph\n    The sampled triplets have only one connection between two concepts in the undirected graph. This avoids ambiguous labels.\n    :param g: the graph\n    :param num_graphs_per_relation: number of triplets to sample per relation. In total len(relations) * num_graphs_per_relation triplets are sampled.\n    :param relations: list of relations to sample for. Train dev and test set are each balanced for these relations.\n    :param seed: random seed\n    :return: a list of triplets, where triplets are encoded by a tuple of two node names\n    :return: a list of the relations between the nodes in the triplets. This are the labels for the relation prediction task.",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "documentation": {}
    },
    {
        "label": "graph_to_list",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "peekOfCode": "def graph_to_list(graph:ig.Graph, triplet:Optional[Tuple[str,str]]=None, label:Optional[str]=None) -> List[Tuple[str,str,str]]:\n    \"\"\"\n    Convert a graph to a list of triplets. \n    :param g: the graph\n    :param triplet: if given, the relation of the triplet is replaced with `<mask>`. triplet is a tuple of two node names.\n    :param label: if given, the label is used to assert that the relation of the triplet is the same as the label. Otherwise it is not used.\n    :return: a list of triplets, where triplets are encoded by a tuple (head, reation, tail)\n    \"\"\"\n    g = graph.copy()\n    if triplet is not None:",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "documentation": {}
    },
    {
        "label": "save_subgraphs",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "peekOfCode": "def save_subgraphs(subgraphs:List[ig.Graph], labels:List[str], r:int, args:Args, seed:int):\n    np.random.seed(seed)\n    train_indices = []\n    dev_indices = []\n    test_indices = []\n    set_labels = list(set(labels))\n    set_labels.sort()  # for reproducibility\n    for label in set_labels:\n        indices = [i for i, l in enumerate(labels) if l == label]\n        np.random.shuffle(indices)",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "peekOfCode": "def main(args):\n    print(\"loading graph\")\n    g = ig.load(args.in_fn, format='pickle')\n    print('getting largest connected component')\n    g = g.clusters().giant()\n    print(g.summary())\n    assert g.is_simple()\n    assert g.is_connected()\n    assert g.is_directed()\n    relations = set(chain(*g.es['relation']))",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_construction",
        "documentation": {}
    },
    {
        "label": "Args",
        "kind": 6,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "peekOfCode": "class Args(Namespace):\n    def __init__(\n            self,\n            kg:str='conceptnet',\n            dataset_construction:str='random',\n            radius:int=1,\n            num_masked:int=0,\n            model:str='t5-small',\n            device:str='cpu',\n        ):",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "documentation": {}
    },
    {
        "label": "get_embedding",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "peekOfCode": "def get_embedding(input, model):\n    e = model(input).last_hidden_state\n    e = e.mean(dim=1).squeeze(dim=0)\n    return e\ndef get_embeddings(graphs:dict, tokenizer, model, old_embeddings=None):\n    if old_embeddings is None:\n        old_embeddings = {}\n    all_tok_dicts = [\n        _get_str2tok(g=graph, tokenizer=tokenizer) for graph_per_split in graphs.values() for graph in graph_per_split\n    ]  # list of dicts of {concept: tokenized concept}",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "documentation": {}
    },
    {
        "label": "get_embeddings",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "peekOfCode": "def get_embeddings(graphs:dict, tokenizer, model, old_embeddings=None):\n    if old_embeddings is None:\n        old_embeddings = {}\n    all_tok_dicts = [\n        _get_str2tok(g=graph, tokenizer=tokenizer) for graph_per_split in graphs.values() for graph in graph_per_split\n    ]  # list of dicts of {concept: tokenized concept}\n    all_tok_dicts = {k: torch.tensor(v, device=args.device).unsqueeze(dim=0) for d in all_tok_dicts for k, v in d.items()}  # merge dicts\n    # remove concepts that are already in old_embeddings\n    all_tok_dicts = {k: v for k, v in all_tok_dicts.items() if k not in old_embeddings.keys()}\n    # todo use batching",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "documentation": {}
    },
    {
        "label": "graph_to_torch_geometric",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "peekOfCode": "def graph_to_torch_geometric(graph:Graph, label, label_to_index:dict, embeddings:dict):\n    concepts = list(graph.concepts)\n    x = torch.stack([embeddings[concept] for concept in concepts], dim=0)\n    concept_to_index = {concept: i for i, concept in enumerate(concepts)}\n    edges = graph.g  # list of (concept1, relation, concept2)\n    neighboring_concepts = [(c1, c2) for c1, r, c2 in edges if r=='<mask>']\n    assert len(neighboring_concepts) == 1\n    neighboring_concepts = neighboring_concepts[0]\n    is_neighbor = [False] * len(concepts)\n    for neighbor in neighboring_concepts:",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "description": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "peekOfCode": "def main(args, old_embeddings=None, old_model_str=None, old_model=None):\n    if old_embeddings is None:\n        old_embeddings = {}\n    else:\n        assert old_model_str == args.model\n    if old_model is not None:\n        assert old_model_str == args.model\n    print('load data')\n    graphs, labels, label_to_index = load_data(kg=args.kg, dataset_construction=args.dataset_construction, radius=args.radius, num_masked=args.num_masked)\n    print('load model')",
        "detail": "GraphLanguageModels.preprocessing.relation_prediction_dataset_to_torch_geometric",
        "documentation": {}
    },
    {
        "label": "get_batch",
        "kind": 2,
        "importPath": "GraphLanguageModels.minimal_working_example",
        "description": "GraphLanguageModels.minimal_working_example",
        "peekOfCode": "def get_batch(data_instances:List[Data], pad_token_id:int, device:str):\n    \"\"\"\n    slightly simplified version of the get_batch in experiments/encoder/train_LM.py\n    \"\"\"\n    max_seq_len = max([data.input_ids.shape[1] for data in data_instances])\n    # intialize tensors\n    input_ids = torch.ones((len(data_instances), max_seq_len), dtype=torch.long, device=device) * pad_token_id\n    relative_position = torch.zeros((len(data_instances), max_seq_len, max_seq_len), dtype=torch.long, device=device)\n    sparsity_mask = torch.zeros((len(data_instances), max_seq_len, max_seq_len), dtype=torch.bool, device=device)\n    use_additional_bucket = torch.zeros((len(data_instances), max_seq_len, max_seq_len), dtype=torch.bool, device=device)",
        "detail": "GraphLanguageModels.minimal_working_example",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.minimal_working_example",
        "description": "GraphLanguageModels.minimal_working_example",
        "peekOfCode": "def main():\n    # define random parameters\n    num_classes = 5\n    modelsize = \"t5-small\"\n    init_additional_buckets_from = 1e6\n    # define test inputs (2 instances to implement batching)\n    graph1 = [\n        (\"dog\", \"is a\", \"animal\"),\n        (\"cat\", \"is a\", \"animal\"),\n        (\"black poodle\", \"is a\", \"dog\"),",
        "detail": "GraphLanguageModels.minimal_working_example",
        "documentation": {}
    },
    {
        "label": "get_batch",
        "kind": 2,
        "importPath": "GraphLanguageModels.minimal_working_example_generate",
        "description": "GraphLanguageModels.minimal_working_example_generate",
        "peekOfCode": "def get_batch(data_instances: List[Data], pad_token_id: int, device: str):\n    \"\"\"\n    slightly simplified version of the get_batch in experiments/encoder/train_LM.py\n    \"\"\"\n    max_seq_len = max([data.input_ids.shape[1] for data in data_instances])\n    # intialize tensors\n    input_ids = (\n        torch.ones((len(data_instances), max_seq_len), dtype=torch.long, device=device)\n        * pad_token_id\n    )",
        "detail": "GraphLanguageModels.minimal_working_example_generate",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "GraphLanguageModels.minimal_working_example_generate",
        "description": "GraphLanguageModels.minimal_working_example_generate",
        "peekOfCode": "def main():\n    # define random parameters\n    modelsize = \"t5-large\"\n    init_additional_buckets_from = 1e6\n    # define test inputs (2 instances to implement batching)\n    graph1 = [\n        (\"dog\", \"is a\", \"animal\"),\n        (\"cat\", \"is a\", \"animal\"),\n        (\"black poodle\", \"is a\", \"dog\"),\n    ]",
        "detail": "GraphLanguageModels.minimal_working_example_generate",
        "documentation": {}
    },
    {
        "label": "HF_CACHE_DIR",
        "kind": 5,
        "importPath": "GraphLanguageModels.minimal_working_example_generate",
        "description": "GraphLanguageModels.minimal_working_example_generate",
        "peekOfCode": "HF_CACHE_DIR = \"/home/students/kolber/seminars/kolber/.cache/\"\nos.environ[\"HF_HOME\"] = HF_CACHE_DIR\nimport torch\nfrom typing import List\nfrom models.graph_T5.autoregressive_GLM import GraphT5ForConditionalGeneration\nfrom models.graph_T5.wrapper_functions import (\n    Graph,\n    graph_to_graphT5,\n    get_embedding,\n    Data,",
        "detail": "GraphLanguageModels.minimal_working_example_generate",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "GraphLanguageModels.minimal_working_example_generate",
        "description": "GraphLanguageModels.minimal_working_example_generate",
        "peekOfCode": "os.environ[\"HF_HOME\"] = HF_CACHE_DIR\nimport torch\nfrom typing import List\nfrom models.graph_T5.autoregressive_GLM import GraphT5ForConditionalGeneration\nfrom models.graph_T5.wrapper_functions import (\n    Graph,\n    graph_to_graphT5,\n    get_embedding,\n    Data,\n    add_text_to_graph_data,",
        "detail": "GraphLanguageModels.minimal_working_example_generate",
        "documentation": {}
    },
    {
        "label": "create_target_prompt",
        "kind": 2,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "def create_target_prompt(graph):\n    target_prompt = \"summarize: \"\n    for triplet in graph:\n        if \"<extra_id_0>\" not in triplet:\n            continue\n        for element in triplet:\n            target_prompt += f\"{element} \"\n    target_prompt = target_prompt.replace(\"<extra_id_0>\", \"?\")\n    return target_prompt.strip()\ndef find_source_position(graph):",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "find_source_position",
        "kind": 2,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "def find_source_position(graph):\n    input_data_target = model.encoder.data_processor.encode_graph(\n        tokenizer=tokenizer,\n        g=graph,\n        how=\"global\",\n    )\n    inp_target = model.encoder.data_processor.to_batch(\n        data_instances=[input_data_target],\n        tokenizer=tokenizer,\n        max_seq_len=None,",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "find_target_position",
        "kind": 2,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "def find_target_position(text):\n    tokenized_text = tokenizer.tokenize(text, add_special_tokens=True)\n    return tokenized_text.index(\"?\")\ndef create_graphs_and_labels(graphs, labels, mask_item=\"subject\"):\n    new_labels = []\n    new_graphs = []\n    triplet_mask_idx = triplet_func_to_idx[mask_item]\n    for graph, label in zip(graphs, labels):\n        new_graph = deepcopy(graph)\n        for triplet_idx, triplet in enumerate(graph):",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "create_graphs_and_labels",
        "kind": 2,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "def create_graphs_and_labels(graphs, labels, mask_item=\"subject\"):\n    new_labels = []\n    new_graphs = []\n    triplet_mask_idx = triplet_func_to_idx[mask_item]\n    for graph, label in zip(graphs, labels):\n        new_graph = deepcopy(graph)\n        for triplet_idx, triplet in enumerate(graph):\n            for element_idx, element in enumerate(triplet):\n                if (\n                    \"<mask>\" not in new_graph[triplet_idx]",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "splits",
        "kind": 5,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "splits = [\"train\", \"dev\", \"test\"]\ndata = {split: {} for split in splits}  #\ntokenizer = AutoTokenizer.from_pretrained(\"plenz/GLM-t5-large\")\nmodel = AutoModel.from_pretrained(\"plenz/GLM-t5-large\", trust_remote_code=True)\nfor split in splits:\n    radius_dict = {radius: {} for radius in range(1, 6)}\n    data[split] = radius_dict\n    for radius in range(1, 6):\n        graphs = []\n        labels = []",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "data = {split: {} for split in splits}  #\ntokenizer = AutoTokenizer.from_pretrained(\"plenz/GLM-t5-large\")\nmodel = AutoModel.from_pretrained(\"plenz/GLM-t5-large\", trust_remote_code=True)\nfor split in splits:\n    radius_dict = {radius: {} for radius in range(1, 6)}\n    data[split] = radius_dict\n    for radius in range(1, 6):\n        graphs = []\n        labels = []\n        with open(",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\"plenz/GLM-t5-large\")\nmodel = AutoModel.from_pretrained(\"plenz/GLM-t5-large\", trust_remote_code=True)\nfor split in splits:\n    radius_dict = {radius: {} for radius in range(1, 6)}\n    data[split] = radius_dict\n    for radius in range(1, 6):\n        graphs = []\n        labels = []\n        with open(\n            f\"/home/students/kolber/seminars/kolber/data/radius={radius}/{split}_graphs.jsonl\",",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "model = AutoModel.from_pretrained(\"plenz/GLM-t5-large\", trust_remote_code=True)\nfor split in splits:\n    radius_dict = {radius: {} for radius in range(1, 6)}\n    data[split] = radius_dict\n    for radius in range(1, 6):\n        graphs = []\n        labels = []\n        with open(\n            f\"/home/students/kolber/seminars/kolber/data/radius={radius}/{split}_graphs.jsonl\",\n            \"r\",",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "triplet_func_to_idx",
        "kind": 5,
        "importPath": "data.create_data",
        "description": "data.create_data",
        "peekOfCode": "triplet_func_to_idx = {\"subject\": 0, \"relation\": 1, \"object\": 2}\ndef create_target_prompt(graph):\n    target_prompt = \"summarize: \"\n    for triplet in graph:\n        if \"<extra_id_0>\" not in triplet:\n            continue\n        for element in triplet:\n            target_prompt += f\"{element} \"\n    target_prompt = target_prompt.replace(\"<extra_id_0>\", \"?\")\n    return target_prompt.strip()",
        "detail": "data.create_data",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "data.load_data",
        "description": "data.load_data",
        "peekOfCode": "def load_data(split: str, radius: int, mask_item: str):\n    df = pd.read_csv(f\"data/radius={radius}/{mask_item}_masked/{split}.csv\")\n    df[\"graph\"] = df[\"graph\"].apply(lambda x: ast.literal_eval(x))\n    ds = Dataset.from_pandas(df)\n    return ds\ntest_data = load_data(\"test\", 1, \"subject\")",
        "detail": "data.load_data",
        "documentation": {}
    },
    {
        "label": "test_data",
        "kind": 5,
        "importPath": "data.load_data",
        "description": "data.load_data",
        "peekOfCode": "test_data = load_data(\"test\", 1, \"subject\")",
        "detail": "data.load_data",
        "documentation": {}
    },
    {
        "label": "prepare_model_and_tokenizer",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "def prepare_model_and_tokenizer(model_name, torch_dtype=None):\n    mt = ModelAndTokenizer(\n        model_name,\n        low_cpu_mem_usage=False,\n        torch_dtype=torch_dtype,\n        device=\"cpu\",\n    )\n    mt.set_hs_patch_hooks = set_hs_patch_hooks_glm\n    mt.model.eval()\n    return mt",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "palette_",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "palette_ = sns.color_palette(\"Set1\")\npalette = palette_[2:5] + palette_[7:]\nsns.set_theme(style=\"whitegrid\")\ntqdm.pandas()\n# Set Hugging Face cache directory\nos.environ[\"HF_HOME\"] = \"/home/students/kolber/seminars/kolber/.cache\"\n# Load model\nglm_model_name = \"plenz/GLM-t5-large\"\ngeneration_model_name = \"google-t5/t5-large\"\ndef prepare_model_and_tokenizer(model_name, torch_dtype=None):",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "palette",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "palette = palette_[2:5] + palette_[7:]\nsns.set_theme(style=\"whitegrid\")\ntqdm.pandas()\n# Set Hugging Face cache directory\nos.environ[\"HF_HOME\"] = \"/home/students/kolber/seminars/kolber/.cache\"\n# Load model\nglm_model_name = \"plenz/GLM-t5-large\"\ngeneration_model_name = \"google-t5/t5-large\"\ndef prepare_model_and_tokenizer(model_name, torch_dtype=None):\n    mt = ModelAndTokenizer(",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "os.environ[\"HF_HOME\"] = \"/home/students/kolber/seminars/kolber/.cache\"\n# Load model\nglm_model_name = \"plenz/GLM-t5-large\"\ngeneration_model_name = \"google-t5/t5-large\"\ndef prepare_model_and_tokenizer(model_name, torch_dtype=None):\n    mt = ModelAndTokenizer(\n        model_name,\n        low_cpu_mem_usage=False,\n        torch_dtype=torch_dtype,\n        device=\"cpu\",",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "glm_model_name",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "glm_model_name = \"plenz/GLM-t5-large\"\ngeneration_model_name = \"google-t5/t5-large\"\ndef prepare_model_and_tokenizer(model_name, torch_dtype=None):\n    mt = ModelAndTokenizer(\n        model_name,\n        low_cpu_mem_usage=False,\n        torch_dtype=torch_dtype,\n        device=\"cpu\",\n    )\n    mt.set_hs_patch_hooks = set_hs_patch_hooks_glm",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "generation_model_name",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "generation_model_name = \"google-t5/t5-large\"\ndef prepare_model_and_tokenizer(model_name, torch_dtype=None):\n    mt = ModelAndTokenizer(\n        model_name,\n        low_cpu_mem_usage=False,\n        torch_dtype=torch_dtype,\n        device=\"cpu\",\n    )\n    mt.set_hs_patch_hooks = set_hs_patch_hooks_glm\n    mt.model.eval()",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "glm_mt",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "glm_mt = prepare_model_and_tokenizer(glm_model_name)\ngeneration_mt = prepare_model_and_tokenizer(generation_model_name)\ntriplet_func_to_idx = {\"subject\": 0, \"relation\": 1, \"object\": 2}\nrecords = []\nfor radius in range(1, 6):\n    for mask_triplet_element in triplet_func_to_idx.keys():\n        split = \"test\"\n        data = load_data(split, radius, mask_triplet_element)\n        for datapoint in data:\n            accuracy = evaluate_patch_glm_accuracy(",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "generation_mt",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "generation_mt = prepare_model_and_tokenizer(generation_model_name)\ntriplet_func_to_idx = {\"subject\": 0, \"relation\": 1, \"object\": 2}\nrecords = []\nfor radius in range(1, 6):\n    for mask_triplet_element in triplet_func_to_idx.keys():\n        split = \"test\"\n        data = load_data(split, radius, mask_triplet_element)\n        for datapoint in data:\n            accuracy = evaluate_patch_glm_accuracy(\n                glm_mt=glm_mt,",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "triplet_func_to_idx",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "triplet_func_to_idx = {\"subject\": 0, \"relation\": 1, \"object\": 2}\nrecords = []\nfor radius in range(1, 6):\n    for mask_triplet_element in triplet_func_to_idx.keys():\n        split = \"test\"\n        data = load_data(split, radius, mask_triplet_element)\n        for datapoint in data:\n            accuracy = evaluate_patch_glm_accuracy(\n                glm_mt=glm_mt,\n                generation_mt=generation_mt,",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "records",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "records = []\nfor radius in range(1, 6):\n    for mask_triplet_element in triplet_func_to_idx.keys():\n        split = \"test\"\n        data = load_data(split, radius, mask_triplet_element)\n        for datapoint in data:\n            accuracy = evaluate_patch_glm_accuracy(\n                glm_mt=glm_mt,\n                generation_mt=generation_mt,\n                source_graph=datapoint[\"graph\"],",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "interpretability.patchscopes.code.GLM_patchscopes",
        "description": "interpretability.patchscopes.code.GLM_patchscopes",
        "peekOfCode": "results = pd.DataFrame.from_records(records)\nresults.to_excel(\"results.xlsx\")",
        "detail": "interpretability.patchscopes.code.GLM_patchscopes",
        "documentation": {}
    },
    {
        "label": "apply_delta",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.apply_delta",
        "description": "interpretability.patchscopes.code.apply_delta",
        "peekOfCode": "def apply_delta(base_model_path, target_model_path, delta_path):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    print(\"Loading delta\")\n    delta = AutoModelForCausalLM.from_pretrained(\n        delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)",
        "detail": "interpretability.patchscopes.code.apply_delta",
        "documentation": {}
    },
    {
        "label": "ModelAndTokenizer",
        "kind": 6,
        "importPath": "interpretability.patchscopes.code.general_utils",
        "description": "interpretability.patchscopes.code.general_utils",
        "peekOfCode": "class ModelAndTokenizer:\n    \"\"\"An object to hold a GPT-style language model and tokenizer.\"\"\"\n    def __init__(\n        self,\n        model_name=None,\n        model=None,\n        tokenizer=None,\n        low_cpu_mem_usage=False,\n        torch_dtype=None,\n        use_fast=True,",
        "detail": "interpretability.patchscopes.code.general_utils",
        "documentation": {}
    },
    {
        "label": "make_inputs",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.general_utils",
        "description": "interpretability.patchscopes.code.general_utils",
        "peekOfCode": "def make_inputs(\n    tokenizer: PreTrainedTokenizer, prompts: List[str], device: str = \"cuda\"\n) -> Dict[str, torch.Tensor]:\n    \"\"\"This function tokenizes a list of text prompts, pads them to a uniform length,\n    creates attention masks, and converts the results into PyTorch tensors suitable for model input.\n    Args:\n        tokenizer (PreTrainedTokenizer): Tokenizer to use.\n        prompts (List[str]): List of prompts.\n        device (str, optional): Device to use. Defaults to \"cuda\".\n    Returns:",
        "detail": "interpretability.patchscopes.code.general_utils",
        "documentation": {}
    },
    {
        "label": "decode_tokens",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.general_utils",
        "description": "interpretability.patchscopes.code.general_utils",
        "peekOfCode": "def decode_tokens(tokenizer, token_array):\n    if hasattr(token_array, \"shape\") and len(token_array.shape) > 1:\n        return [decode_tokens(tokenizer, row) for row in token_array]\n    return [tokenizer.decode([t]) for t in token_array]\ndef find_token_range(tokenizer, token_array, substring):\n    \"\"\"Find the tokens corresponding to the given substring in token_array.\"\"\"\n    tokens = decode_tokens(tokenizer, token_array)\n    whole_string = \"\".join(tokens)\n    char_loc = whole_string.index(substring)\n    loc = 0",
        "detail": "interpretability.patchscopes.code.general_utils",
        "documentation": {}
    },
    {
        "label": "find_token_range",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.general_utils",
        "description": "interpretability.patchscopes.code.general_utils",
        "peekOfCode": "def find_token_range(tokenizer, token_array, substring):\n    \"\"\"Find the tokens corresponding to the given substring in token_array.\"\"\"\n    tokens = decode_tokens(tokenizer, token_array)\n    whole_string = \"\".join(tokens)\n    char_loc = whole_string.index(substring)\n    loc = 0\n    tok_start, tok_end = None, None\n    for i, t in enumerate(tokens):\n        loc += len(t)\n        if tok_start is None and loc > char_loc:",
        "detail": "interpretability.patchscopes.code.general_utils",
        "documentation": {}
    },
    {
        "label": "predict_from_input",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.general_utils",
        "description": "interpretability.patchscopes.code.general_utils",
        "peekOfCode": "def predict_from_input(model, inp):\n    out = model(**inp)[\"logits\"]\n    probs = torch.softmax(out[:, -1], dim=1)\n    p, preds = torch.max(probs, dim=1)\n    return preds, p\ndef set_requires_grad(requires_grad, *models):\n    for model in models:\n        if isinstance(model, torch.nn.Module):\n            for param in model.parameters():\n                param.requires_grad = requires_grad",
        "detail": "interpretability.patchscopes.code.general_utils",
        "documentation": {}
    },
    {
        "label": "set_requires_grad",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.general_utils",
        "description": "interpretability.patchscopes.code.general_utils",
        "peekOfCode": "def set_requires_grad(requires_grad, *models):\n    for model in models:\n        if isinstance(model, torch.nn.Module):\n            for param in model.parameters():\n                param.requires_grad = requires_grad\n        elif isinstance(model, (torch.nn.Parameter, torch.Tensor)):\n            model.requires_grad = requires_grad\n        else:\n            assert False, \"unknown type %r\" % type(model)",
        "detail": "interpretability.patchscopes.code.general_utils",
        "documentation": {}
    },
    {
        "label": "set_hs_patch_hooks_glm",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def set_hs_patch_hooks_glm(\n    model,\n    hs_patch_config,\n    module=\"hs\",  # mlp, attn\n    patch_input=False,\n    skip_final_ln=False,\n    generation_mode=False,\n):\n    \"\"\"T5 patch hooks.\"\"\"\n    # when using mode.generate() the hidden states in the input are cached after",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "set_hs_patch_hooks_neox",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def set_hs_patch_hooks_neox(\n    model,\n    hs_patch_config,\n    module=\"hs\",  # mlp, attn\n    patch_input=False,\n    skip_final_ln=False,\n    generation_mode=False,\n):\n    \"\"\"Neox patch hooks.\"\"\"\n    # when using mode.generate() the hidden states in the input are cached after",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "set_hs_patch_hooks_llama",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def set_hs_patch_hooks_llama(\n    model,\n    hs_patch_config,\n    module=\"hs\",  # mlp, attn\n    patch_input=False,\n    skip_final_ln=False,\n    generation_mode=False,\n):\n    \"\"\"Llama patch hooks.\"\"\"\n    # when using mode.generate() the hidden states in the input are cached after",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "set_hs_patch_hooks_gptj",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def set_hs_patch_hooks_gptj(\n    model,\n    hs_patch_config,\n    module=\"hs\",  # mlp, attn\n    patch_input=False,\n    skip_final_ln=False,\n    generation_mode=False,\n):\n    \"\"\"GPTJ patch hooks.\"\"\"\n    # when using mode.generate() the hidden states in the input are cached after",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "remove_hooks",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def remove_hooks(hooks):\n    for hook in hooks:\n        hook.remove()\n# ##############\n#\n# Inspection\n#\n# ##############\ndef inspect(\n    mt,",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def inspect(\n    mt,\n    prompt_source,\n    prompt_target,\n    layer_source,\n    layer_target,\n    position_source,\n    position_target,\n    module=\"hs\",\n    generation_mode=False,",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "evaluate_patch_glm",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def evaluate_patch_glm(\n    glm_mt: ModelAndTokenizer,\n    generation_mt: ModelAndTokenizer,\n    how: str,\n    layer_source: int,\n    layer_target: int,\n    position_source: int,\n    position_target: int,\n    source_graph: str = None,\n    source_text: str = None,",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "evaluate_patch_glm_accuracy",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def evaluate_patch_glm_accuracy(\n    glm_mt: ModelAndTokenizer,\n    generation_mt: ModelAndTokenizer,\n    how: str,\n    position_source: int,\n    position_target: int,\n    target_label: str,\n    source_graph: str = None,\n    source_text: str = None,\n    target_graph: str = None,",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "evaluate_patch_next_token_prediction",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def evaluate_patch_next_token_prediction(\n    mt,\n    prompt_source,\n    prompt_target,\n    layer_source,\n    layer_target,\n    position_source,\n    position_target,\n    module=\"hs\",\n    position_prediction=-1,",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "evaluate_patch_next_token_prediction_x_model",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def evaluate_patch_next_token_prediction_x_model(\n    mt_1,\n    mt_2,\n    prompt_source,\n    prompt_target,\n    layer_source,\n    layer_target,\n    position_source,\n    position_target,\n    module=\"hs\",",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "set_hs_patch_hooks_gptj_batch",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def set_hs_patch_hooks_gptj_batch(\n    model,\n    hs_patch_config,\n    module=\"hs\",\n    patch_input=False,\n    generation_mode=False,\n):\n    \"\"\"GPTJ patch hooks - supporting batch.\"\"\"\n    # when using mode.generate() the hidden states in the input are cached after\n    # the first inference pass, and in the next steps the input/output are of",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "set_hs_patch_hooks_llama_batch",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def set_hs_patch_hooks_llama_batch(\n    model,\n    hs_patch_config,\n    module=\"hs\",\n    patch_input=False,\n    generation_mode=False,\n):\n    \"\"\"LLAMA patch hooks - supporting batch.\"\"\"\n    # when using mode.generate() the hidden states in the input are cached after\n    # the first inference pass, and in the next steps the input/output are of",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "evaluate_patch_next_token_prediction_batch",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def evaluate_patch_next_token_prediction_batch(\n    mt, df, batch_size=256, transform=None, module=\"hs\"\n):\n    \"\"\"Evaluate next token prediction with batch support.\"\"\"\n    if module != \"hs\":\n        raise ValueError(\"Module %s not yet supported\", module)\n    prec_1 = np.zeros(0)\n    surprisal = np.zeros(0)\n    next_token = np.zeros(0)\n    #     generations = []",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "inspect_batch",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def inspect_batch(mt, df, batch_size=256, transform=None, module=\"hs\"):\n    \"\"\"Inspects batch: source/target layer/position could differ within batch.\"\"\"\n    if module != \"hs\":\n        raise ValueError(\"Module %s not yet supported\", module)\n    generations = []\n    def _inspect_single_batch(batch_df):\n        batch_size = len(batch_df)\n        prompt_source_batch = np.array(batch_df[\"prompt_source\"])\n        prompt_target_batch = np.array(batch_df[\"prompt_target\"])\n        layer_source_batch = np.array(batch_df[\"layer_source\"])",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "evaluate_attriburte_exraction_batch",
        "kind": 2,
        "importPath": "interpretability.patchscopes.code.patchscopes_utils",
        "description": "interpretability.patchscopes.code.patchscopes_utils",
        "peekOfCode": "def evaluate_attriburte_exraction_batch(\n    mt,\n    df,\n    batch_size=256,\n    max_gen_len=10,\n    transform=None,\n    is_icl=True,\n    module=\"hs\",\n):\n    \"\"\"Evaluates attribute extraction with batch support.\"\"\"",
        "detail": "interpretability.patchscopes.code.patchscopes_utils",
        "documentation": {}
    },
    {
        "label": "create_token_list",
        "kind": 2,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "def create_token_list(fp):\n    token_list = []\n    with open(\"/home/students/kolber/seminars/kolber/token_list.txt\", \"r\") as file:\n        for line in file:\n            token_list.append(line.strip())\n    return token_list\nHF_CACHE_DIR = \"/home/students/kolber/seminars/kolber/.cache/\"\nN = 10000\nARROW = \"=>\"\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)",
        "detail": "t5_experiments",
        "documentation": {}
    },
    {
        "label": "HF_CACHE_DIR",
        "kind": 5,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "HF_CACHE_DIR = \"/home/students/kolber/seminars/kolber/.cache/\"\nos.environ[\"HF_HOME\"] = HF_CACHE_DIR\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    T5ForConditionalGeneration,\n)\nimport os\nimport random\ndef create_token_list(fp):",
        "detail": "t5_experiments",
        "documentation": {}
    },
    {
        "label": "os.environ[\"HF_HOME\"]",
        "kind": 5,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "os.environ[\"HF_HOME\"] = HF_CACHE_DIR\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    T5ForConditionalGeneration,\n)\nimport os\nimport random\ndef create_token_list(fp):\n    token_list = []",
        "detail": "t5_experiments",
        "documentation": {}
    },
    {
        "label": "HF_CACHE_DIR",
        "kind": 5,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "HF_CACHE_DIR = \"/home/students/kolber/seminars/kolber/.cache/\"\nN = 10000\nARROW = \"=>\"\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntokenizer = AutoTokenizer.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntoken_list = create_token_list(\"/home/students/kolber/seminars/kolber/token_list.txt\")\nfor i in range(N):\n    token1 = random.choice(token_list)\n    token2 = random.choice(token_list)\n    accuracy = 0",
        "detail": "t5_experiments",
        "documentation": {}
    },
    {
        "label": "N",
        "kind": 5,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "N = 10000\nARROW = \"=>\"\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntokenizer = AutoTokenizer.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntoken_list = create_token_list(\"/home/students/kolber/seminars/kolber/token_list.txt\")\nfor i in range(N):\n    token1 = random.choice(token_list)\n    token2 = random.choice(token_list)\n    accuracy = 0\n    for j in range(10):",
        "detail": "t5_experiments",
        "documentation": {}
    },
    {
        "label": "ARROW",
        "kind": 5,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "ARROW = \"=>\"\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntokenizer = AutoTokenizer.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntoken_list = create_token_list(\"/home/students/kolber/seminars/kolber/token_list.txt\")\nfor i in range(N):\n    token1 = random.choice(token_list)\n    token2 = random.choice(token_list)\n    accuracy = 0\n    for j in range(10):\n        token3 = random.choice(token_list)",
        "detail": "t5_experiments",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntokenizer = AutoTokenizer.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntoken_list = create_token_list(\"/home/students/kolber/seminars/kolber/token_list.txt\")\nfor i in range(N):\n    token1 = random.choice(token_list)\n    token2 = random.choice(token_list)\n    accuracy = 0\n    for j in range(10):\n        token3 = random.choice(token_list)\n        inputs = tokenizer(",
        "detail": "t5_experiments",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\", cache_dir=HF_CACHE_DIR)\ntoken_list = create_token_list(\"/home/students/kolber/seminars/kolber/token_list.txt\")\nfor i in range(N):\n    token1 = random.choice(token_list)\n    token2 = random.choice(token_list)\n    accuracy = 0\n    for j in range(10):\n        token3 = random.choice(token_list)\n        inputs = tokenizer(\n            f\"\"\"summarize: {token1}{ARROW}{token1};{token2}{ARROW}{token2};{token3}{ARROW}",
        "detail": "t5_experiments",
        "documentation": {}
    },
    {
        "label": "token_list",
        "kind": 5,
        "importPath": "t5_experiments",
        "description": "t5_experiments",
        "peekOfCode": "token_list = create_token_list(\"/home/students/kolber/seminars/kolber/token_list.txt\")\nfor i in range(N):\n    token1 = random.choice(token_list)\n    token2 = random.choice(token_list)\n    accuracy = 0\n    for j in range(10):\n        token3 = random.choice(token_list)\n        inputs = tokenizer(\n            f\"\"\"summarize: {token1}{ARROW}{token1};{token2}{ARROW}{token2};{token3}{ARROW}\n            \"\"\",",
        "detail": "t5_experiments",
        "documentation": {}
    }
]